\documentclass[12pt] {article}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{fullpage}
\usepackage{amsmath}
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}
\usepackage{pslatex}
\renewcommand{\baselinestretch}{2}
\usepackage{setspace}
\usepackage[nottoc]{tocbibind} 
\usepackage{authblk}
\usepackage{rotating}
\usepackage{adjustbox}
\usepackage{appendix}


\begin{document}
\title{Manual of Best Practices in Transparent Social Science Research}

\author{Garret Christensen\footnote{UC Berkeley, Berkeley Initiative for Transparency in the Social Sciences. Please send correspondence to garret@berkeley.edu, or find the version controlled history of this document on \href{https://github.com/garretchristensen/BestPracticesManual}{github}. I gratefully acknowledge helpful comments from Jennifer Sturdy, Alex Wais, and Courtney Soderberg, as well as participants in BITSS seminars and workshops. Funding for this manuscript was provided by an anonymous donor who play no role in writing or reviewing the manuscript, nor in the decision to publish.}}
%\affil[1]{UC Berkeley, Berkeley Initiative for Transparency in the Social Sciences}
\date{\today}
\maketitle

%\begin{center}
%Comments and suggestions are strongly encouraged. Please send correspondence to garret@berkeley.edu, or find the latest version of the manual on \href{https://github.com/garretchristensen/BestPracticesManual}{github}.
%\end{center}

\newpage
\tableofcontents

\newpage
\section{Introduction}\label{introduction}

The principle that scientific claims should be subject to scrutiny by other researchers and the public at large is well established.  Just examine the Royal Society's motto \textit{nullius in verba} (``take nobody's word for it'') or seminal works on the sociology of science such as  \cite{merton1973sociology}. An important requirement for such scrutiny is that
researchers make their claims transparent in a way that other
researchers are able to use easily available resources to form a
complete understanding of the methods that were used by the original. In
the social sciences, especially given the personal computing and
Internet revolutions and the wide availability of data and processing
power, it is essential that data, code, and analyses be transparent.

This manual is intended to be a source mainly for economists and other empirical social
science researchers who desire to make their own research transparent
to, and reproducible by, others. The entire process of research, from
hypothesis generation to publication, is covered. Although norms differ
across disciplines, and this is focused on economics, I attempt to bring a broad view of the empirical
social sciences to these recommendations, and hope that students and
researchers in any social science field may tailor these recommendations
to best fit their field.

The manual is laid out as follows: in section~\ref{ethical-research} I first discuss the motivation for this document: the
desire to do ethical research. A major component of ethical social
science research is treating research subjects appropriately. This is
mandated by federal law and overseen by Institutional Review Boards
(IRBs), and should be taken seriously by researchers. But just as
treating subjects fairly is ethical, transparent,
reproducible research is also a major part of ethical research.

In section~\ref{study-design} I discuss study design, including how to power studies
appropriately.

In section~\ref{registration} I discuss one of the major problems in non-transparent
research, specifically publication bias. I also discuss how this
problem can be resolved through the practice of registration.
Publication bias stems from the fact that published results are
overwhelmingly statistically significant. But without knowing how many
tests were run (the number of unpublished results), it is impossible to know whether these significant
results are meaningful, or whether they are the 5\% of tests that we
would expect to appear significant due to random sampling, even with no
true effect. By publicly registering all studies, we can have a better
idea of just how many tests have been run.

In section~\ref{rdof} I discuss researcher degrees of freedom and pre-analysis
plans; In addition to registering trials, researchers can also
specify their outcomes of interest and their exact methods of analysis
to bind their hands during the analysis phase by writing a Pre-Analysis
Plan (PAP). This is a relatively new idea in the social sciences, so
there is not yet a consensus on when a PAP should be required, what the
ideal level of detail is, and how much it should constrain a
researcher's hands in the actual analysis, but by pre-specifying analyses,
researchers can distinguish between confirmatory and exploratory
analysis. I do not necessarily place higher intrinsic value on one or
the other, but making the distinction clear is key for appropriate
interpretation.

In section~\ref{replication-and-reproducibility} I discuss workflow and materials sharing, with an eye on
making research replicable by others. Researchers should make their code
and data publicly available so that others may repeat and verify their
analysis. Making data available incentivizes researchers to make their
work accurate in the first place, and makes replication easier for
others, improving the scientific process, but also raises the concern of
differential privacy, since steps should be taken to prevent
identification of individuals in the data. I also discuss the issue of
reporting standards: a standardized list of things that authors should
report to help make their work reproducible.

Section~\ref{conclusion} concludes and presents a vision for moving forward.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ethical Research}\label{ethical-research}

Making one's research transparent and reproducible is a key component of ethical research. Not engaging in fraud is an obvious component of this.

\subsection{Fraud}\label{fraud}

While most of us are likely to presume that we ourselves would not
conduct outright fraud, fraud does indeed occur. From making up fake
data to creating bogus e-mail addresses so one could do one's own peer
review, the \href{http://www.retractionwatch.com}{Retraction Watch blog}
documents a distressingly large amount of deliberate fraud in research.
Although the blog tends to specialize in the life sciences, and there is significantly less money involved in the social sciences than in medical and pharmaceutical research, there is no reason to believe that social science researchers are inherently
more benevolent. Uri Simonsohn, Leif Nelson, and Joseph Simmons used statistical methods to detect fraud in the research of a pair of prominent social psychologists \citep{simonsohn2013just}. 

Another source for information on fraud is part of the US Department of Health and Human Services
, the Office of Research Integrity (ORI), which works to promote research
integrity and document misconduct, especially when it involves federally
funded research. The \href{http://ori.hhs.gov/case_summary}{misconduct case summaries of the ORI}, and the stories of Diederik Stapel \citep{carey_noted_2011, bhattacharjee_diederik_2013} Hwang Woo-Suk \citep{cyranoski_cloning_2014}, Marc Hauser \citep{johnson_harvard_2012} and Michael LaCour \citep{broockman2015irreg} should be sobering warnings to us all.

\subsection{Unintentional Bias}\label{unintentional-bias}

Perhaps in addition to the obvious need to avoid deliberate fraud and
protect our human subjects is the need to avoid subconsciously biasing
our own results.

\cite{nosek_scientific_2012} summarize some of the evidence on this
subject, concluding that there are many circumstances common to academia
and the publishing paradigm that cause researchers to frequently use
motivated reasoning:

\begin{quote}
Because we have directional goals for success, we are likely to bring to
bear motivated reasoning to justify research decisions in the name of
accuracy, when they are actually in service of career advancement
(Fanelli, 2010a). Motivated reasoning is particularly influential when
the situation is complex, the available information is ambiguous, and
legitimate reasons can be generated for multiple courses of action
(Bersoff, 1999; Boiney, Kennedy, \& Nye, 1997; Kunda, 1990).

Motivated reasoning can occur without intention. We are more likely to
be convinced that our hypothesis is true, accepting uncritically when it
is confirmed and crutinizing heavily when it is not (Bastardi, Uhlmann,
\& Ross, 2011; Ditto \& Lopez, 1992; Lord, Ross, \& Lepper, 1979;
Pyszczynski \& Greenberg, 1987; Trope \& Bassok, 1982). With flexible
analysis options, we are more likely to find the one that produces a
more publishable pattern of results to be more reasonable and defensible
than others (Simmons et al., 2011; Wagenmakers, Wetzels, Borsboom, \&
van der Maas, 2011). Once we obtain an unexpected result, we are likely
to reconstruct our histories and perceive the outcome as something that
we could have, even did, anticipate all along---converting a discovery
into a confirmatory result (Fischoff, 1977; Fischoff \& Beyth, 1975).
And even if we resist those reasoning biases in the moment, after a few
months, we might simply forget the details, whether we had hypothesized
the moderator, had good justification for one set of exclusion criteria
compared with another, and had really thought that the one dependent
variable that showed a significant effect was the key outcome. Instead,
we might remember the gist of what the study was and what we found
(Reyna \& Brainerd, 1995). Forgetting the details provides an
opportunity for reimagining the study purpose and results to recall and
understand them in their best (i.e., most publishable) light. The reader
may, as we do, recall personal examples of such motivated
decisions---they are entirely ordinary products of human cognition.
\end{quote}

\subsection{Institutional Review
Boards}\label{institutional-review-boards}

In addition to fraud, a major ethical concern relates to our human
subjects.

\subsubsection{History}\label{history}

World history is rife with examples of atrocities conducted in the name
of research. Some of these have resulted in major changes in regulations
related to research.

\paragraph{Nuremberg}\label{nuremberg}

Nazi German doctors conducted horrible experiments on subjects during
World War II. The ``Doctor's Trial'' (USA v. Karl Brandt, et al.) tried
23 defendants, and the verdict included ten principles regarding voluntary consent, societal benefits from the research, minimizing risk, etc.
which although never entered as formal regulations in either Germany or
the USA, became widely accepted.

%\begin{enumerate}
%\def\labelenumi{\arabic{enumi}.}
%\item
%  The voluntary consent of the human subject is absolutely essential.

%  This means that the person involved should have legal capacity to give consent; should be so situated as to be able to exercise free power of choice, without the intervention of any element of force, fraud,  deceit, duress, over-reaching, or other ulterior form of constraint or coercion; and should have sufficient knowledge and comprehension of the elements of the subject matter involved as to enable him to make an understanding and enlightened decision. This latter element requires that before the acceptance of an affirmative decision by the experimental subject there should be made known to him the nature, duration, and purpose of the experiment; the method and means by which it is to be conducted; all inconveniences and hazards reasonably to be expected; and the effects upon his health or person which may possibly come from his participation in the experiment.

 % The duty and responsibility for ascertaining the quality of the consent rests upon each individual who initiates, directs or engages in the experiment. It is a personal duty and responsibility which may not be delegated to another with impunity.
%\item
 % The experiment should be such as to yield fruitful results for the good of society, unprocurable by other methods or means of study, and  not random and unnecessary in nature.
%\item
 % The experiment should be so designed and based on the results of animal experimentation and a knowledge of the natural history of the disease or other problem under study that the anticipated results will  justify the performance of the experiment.
%\item
 % The experiment should be so conducted as to avoid all unnecessary
  %physical and mental suffering and injury.
%\item
 % No experiment should be conducted where there is an a priori reason to believe that death or disabling injury will occur; except, perhaps, in  those experiments where the experimental physicians also serve as  subjects.
%\item
 % The degree of risk to be taken should never exceed that determined by the humanitarian importance of the problem to be solved by the experiment.
%\item
% Proper preparations should be made and adequate facilities provided to protect the experimental subject against even remote possibilities of injury, disability, or death.
%\item
 % The experiment should be conducted only by scientifically qualified persons. The highest degree of skill and care should be required through all stages of the experiment of those who conduct or engage in the experiment.
%\item
 % During the course of the experiment the human subject should be at liberty to bring the experiment to an end if he has reached the  physical or mental state where continuation of the experiment seems to him to be impossible.
%\item
 % During the course of the experiment the scientist in charge must be prepared to terminate the experiment at any stage, if he has probably cause to believe, in the exercise of the good faith, superior skill and careful judgment required of him that a continuation of the experiment is likely to result in injury, disability, or death to the experimental subject.
%\end{enumerate}

\paragraph{Tuskegee and US
codification}\label{tuskegee-and-us-codification}

In 1972 whistleblower Peter Buxton revealed to the Associated Press that
the US Public Health Service was conducting a 40-year experiment on poor
Alabama sharecroppers in which it did not treat those who had syphilis
for the disease despite the discovery and verification of penicillin as
an effective treatment, and prevented sufferers from obtaining
treatment elsewhere. As a result, the National Commission for the
Protection of Human Subjects of Biomedical and Behavioral Research was
formed by law in 1974, and released the Belmont Report in 1979. \href{http://www.hhs.gov/ohrp/humansubjects/guidance/belmont.html}{The
Belmont Report} contains three basic ethical principles, and three applications:

\begin{itemize}
\item
  Ethical Principles

  \begin{itemize}
  \item
    Respect for Persons: ``Respect for persons incorporates at least two
    ethical convictions: first, that individuals should be treated as
    autonomous agents, and second, that persons with diminished autonomy
    are entitled to protection.''
  \item
    Beneficence: ``Two general rules have been formulated as
    complementary expressions of beneficent actions in this sense:
    \textbf{(1)} do not harm and \textbf{(2)} maximize possible benefits
    and minimize possible harms.''
  \item
    Justice: ``An injustice occurs when some benefit to which a person
    is entitled is denied without good reason or when some burden is
    imposed unduly. Another way of conceiving the principle of justice
    is that equals ought to be treated equally.''
  \end{itemize}
\item
  Applications

  \begin{itemize}
  \item
    Informed Consent: ``Respect for persons requires that subjects, to
    the degree that they are capable, be given the opportunity to choose
    what shall or shall not happen to them. This opportunity is provided
    when adequate standards for informed consent are satisfied.''
  \item
    Assessment of Risks and Benefits: ``It is commonly said that
    benefits and risks must be `balanced' and shown to be `in a
    favorable ratio.' The metaphorical character of these terms draws
    attention to the difficulty of making precise judgments. Only on
    rare occasions will quantitative techniques be available for the
    scrutiny of research protocols. However, the idea of systematic,
    nonarbitrary analysis of risks and benefits should be emulated
    insofar as possible."
  \item
    Selection of Subjects: ``Individual justice in the selection of
    subjects would require that researchers exhibit fairness: thus, they
    should not offer potentially beneficial research only to some
    patients who are in their favor or select only `undesirable' persons
    for risky research. Social justice requires that distinction be
    drawn between classes of subjects that ought, and ought not, to
    participate in any particular kind of research, based on the ability
    of members of that class to bear burdens and on the appropriateness
    of placing further burdens on already burdened persons."
  \end{itemize}
\end{itemize}

In 1981 the Department of Health and Human Services and the Food and
Drug Administration adopted regulations in line with the Belmont report,
and 15 federal agencies adopted these regulations (45 CFR part 46) as
the ``\href{http://www.hhs.gov/ohrp/index.html}{Common Rule}'' in 1991. 

In practice, this means that researchers who receive funding from the US
government, or who work at institutions that receive federal funding should have their research approved by an Institutional Review Board (IRB). IRB are a decentralized approval
body set up by each research organization itself, consisting of at least
five members, a mix of men and women, scientists and non-scientists, and
at least one member not affiliated with the institution. Since IRBs and
the approval process are decentralized, the exact process varies from
institution to institution, but one example can be seen at
\url{http://cphs.berkeley.edu}.

When conducting research internationally, researchers should give their
human subjects the same protections as those inside the US. Laws in
developing countries may not be as well-defined or enforced, but
researchers should still register with their US institution's IRB, and
obtain approval from the host country government. A list of laws and
regulations that cover research in 107 foreign countries is available
from the \href{http://www.hhs.gov/ohrp/international/intlcompilation/2014intlcomp.pdf.pdf}
{Office for Human Research Protections}.

Another key resource for researchers and research conducted outside the
US is the \href{http://www.wma.net/en/30publications/10policies/b3/index.html}{Declaration of Helsinki} by the World Medical Association
(WMA). Originally adopted by the WMA in 1964, the document has significantly
influenced the laws and regulations adopted to govern research
worldwide.

Lest one think that ethical concerns are limited to monsters of bygone
eras, I refer readers to a dilemma caused by a Montana state election experiment by
researchers from \href{http://www.washingtonpost.com/blogs/monkey-cage/wp/2014/11/03/ethics-and-research-in-comparative-politics/}{Stanford and Dartmouth} in 2014, who sent 100,000 people official-looking election flyers bearing the state seal weeks before the election.

\subsubsection{Training}\label{training}

A large number of universities participate in the \href{https://www.citiprogram.org/}{Collaborative Institutional Training Initiative} at the University of Miami (CITI). Completing their course on Human
Subjects Research is often a requirement of being included on a research
proposal. For anyone at an institution not affiliated with CITI, the NIH maintains an \href{https://phrp.nihtraining.com/users/login.php}{online training course} that is free and open to the public.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Study Design}\label{study-design}
There are many issues involving study design that are somewhat related to transparent research. For randomized trials, I refer readers to two excellent resources, \cite{randomizationtoolkit} and \cite{glennerster_running_2013}, which cover many aspects of how to design and implement an excellent field trial. For studies that are not necessarily randomized trials, I recommend \cite{gertler_impact_2011} or \cite{angrist2008mostly} for those with more statistics training. Here I briefly discuss one aspect of study design especially relevant for reproducible research: determining sample size through power calculations.
  
\subsection{Power Analysis}\label{Conducting Power Analysis}
%By CS
Given that researchers are working within a null hypothesis testing framework, the power€™ 
of a study, the probability of rejecting the null hypothesis when it is false, is extremely 
important. Though 80\% power is held as a lower bound for acceptable power in many disciplines 
such as medical research, the actual power of studies can be much lower. For example, research by \cite{ioannidis2017power} found that the median power in economics is 18\%, and that half of research areas in economics have nearly 90\% of their results under-powered. \cite{button_power_2013} found that the median power in neuroscience was 21\%. This means that 
if a study investigating a true effect were run 100 times, 79 studies would fail to reach significance, 
meaning that the chance of a false negative is extremely high. 

Though false negatives are perhaps the most often discussed issue with low powered studies, they are not the only issue. A second issue is that low powered studies actually decrease the likelihood of a true positive \citep{ioannidis_why_2005, button_power_2013}. 
This means that when a low powered studies does find an effect, the lower the likelihood that this effect is true in 
the population; it is relatively more likely that it is a false positive.  A third issue with low powered studies relates to effect 
sizes. Small, low powered studies that reach statistical significance will over-estimate the true effect size in the population \citep{button_power_2013}. These inflated effect size estimates will make it difficult for others to properly power future 
studies, and the inaccuracy of the estimate may also be problematic for making decisions based on scientific results. 
 
An obvious way to help mitigate the problems of underpowered studies is to run studies with higher power. However, this 
process is not always that straightforward. As previously mentioned, effect sizes from published studies are often inflated, 
and so may not provide the most accurate estimate. Meta-analyses can provide better information, but they also often suffer 
from publication bias and thus inflated effect sizes. Additionally, effect sizes may be highly heterogeneous between studies, 
even studies with the same materials and methodologies \citep{many_labs}. Thus, using a single point estimate of an effect size 
from published literature may lead to inaccurate power calculations.

To combat these problems, alternatives to the standard power analysis have been suggested. For example, \cite{perugini2014safeguard} 
have suggested a technique called `safeguard power' which takes into account the uncertainty surrounding effect size estimates 
when conducting power analyses. Specifically, they suggest basing power calculations off of the effect size corresponding to the lower bound of a 60\% confidence interval around the point estimate from published literature. Another approach by \cite{mcshane_2014} takes into account the between study variation in effect sizes when conducting power analyses to give a more conservative estimate of the number of participants needed to reach a given level of statistical power. Yet another approach, which is also feasible when an effect size estimate from the previous literature is not available, is to determine the smallest effect size you wish to be able to detect, and power your study to find this effect size \citep{bloom1995minimum}.  

%GSC
Additionally, researchers may be able to help one another as well as decrease false-positives by publishing or clearly making available their power calculations. If a reader of a paper saw a clearly stated ``this trial was powered to detect an effect size of X,'' this would help to put the study into the appropriate statistical context. The parametric assumptions involved (such as the intra-cluster correlation) could also be publicly posted in study protocols to help other researchers learn reasonable assumptions for their own studies. 
\subsection {Practical Considerations}
The impact of most of these calculations will in many cases mean a larger sample size than some disciplines are used to working with. There are several ways to mitigate the impact of this. 

In psychology, a few of these possible solutions are presented in \cite{openmaximizing}. Researchers can join crowd-sourced projects where multiple labs share protocols and produce the same experiment, as was done with 13 psychological effects and 36 samples and settings in \cite{many_labs}, or with the organized \href{https://osf.io/view/StudySwap/}{Study Swap}. The \href{https://osf.io/wfc6u/}{Collaborative Replications and Education Project} (CREP) tracks findings that can be relatively easily replicated in a teaching setting. 

Economists have also discusses how project budgets should play into study design, by maximizing power subject to a budget constraint \cite{randomizationtoolkit}. When data collection is the main cost, the proportion of treatment and control should each be one half, but if treatment is expensive, power can be maximized subject to the project budget using the rule: ``the ratio of subjects in the treatment group to those in the comparison should be proportional to the inverse of the square root of their cost.''  Another important contribution to this are of power maximization subject to a budget constraint is that power depends on the number of rounds of surveying, and also the autocorrelation of measurements over time. \cite{mckenzie2012caseformoreT} shows that baseline data is of most power when autocorrelation is high, and repeated measurements post-treatment of most power when autocorrelation is low.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Registration}\label{registration}

One of the problems brought into focus recently is publication bias.
Publication bias is the selective publication of only significant
results. Thankfully, there are tools available for researchers to combat
these problems.

\subsection{Publication Bias}\label{publication-bias}

One of the primary drivers of the recent move towards transparency is
increased awareness of publication bias. Numerous papers use collections
of published papers to show that the proportion of significant results
are extremely unlikely to come from any true population distribution
\citep{delong_are_1992, gerber_testing_2001,  ioannidis_why_2005}. By examining the publication rates of null results and
significant results from a large set of NSF-funded studies, \cite{franco_publication_2014} show that the selective publication of only
significant results may stem from the fact that social science
researchers largely fail to write up and submit results from studies
resulting in null findings, citing lack of interest or fear of
rejection. This idea of rejecting, or not even submitting for review, papers with null-results, is commonly referred to as the ``file drawer problem'' \citep{rosenthal1979file}. In fact, the percentage of null findings published in
journals appears to have been decreasing over time, across all
disciplines \citep{fanelli_negative_2012}. It seems unlikely that this would
be an accurate reflection of the state of the universe, unless the hypotheses that scientists are testing are systematically changing over time. If journals only
publish statistically significant results, we have no idea how many of
those significant results are evidence of real effects, and which are
the 5\% of random draws that we should expect to show a significant
result with a true zero effect. One way to combat this problem is to
require registration of all studies undertaken. Ideally we then search
the registry for studies of X on Y. If numerous studies show an
effect, we have confidence the effect is real. If 5\% of studies show a
significant effect, we give these outlier studies less credence.

\subsection{Trial Registration}\label{trial-registration}

A basic definition of registration is to publicly declare \emph{all}
research that one plans on conducting. Ideally this is done in a public
registry designed to accept registrations in the given research discipline,
and ideally the registration takes place before data collection begins.

Registration of randomized trials has achieved wide adoption in medicine, but is still
relatively new to the social sciences. After congress passed a law in
1997 requiring the creation of a registry for FDA-regulated trials, and
the NIH created clinicaltrials.gov in 2000, The International Committee
of Medical Journal Editors (ICMJE), a collection of editors of top
medical journals, instituted a policy of publishing only registered
trials in 2005 \citep{DeAngelis2004}, and the policy has spread to
other journals and been generally accepted by researchers \citep{laine_clinical_2007}. Several other countries have their own national trial registries, and the World Health Organization created the \href{http://www.who.int/ictrp/about/en/}{International Clinical Trials Registry Platform (ICTRP)} in 2007 to automatically collect all this information in one place.

A profound example of the benefit of trial registries is detailed in
\cite{turner_selective_2008}, which details the publication rates of studies
related to FDA-approved antidepressants. (See also \cite{ioannidis_effectiveness_2008}.)
The outcome is perhaps what the most hardened cynic would expect:
essentially all the trials with positive outcomes were published, a
50/50 mix of questionable-outcome studies were published, and a majority of the
negative-outcome studies were unpublished a minimum of four years after the
study was completed. %The figure below shows the drastically different
%rates of publication, and a large amount of publication bias.
%\begin{center}
%\includegraphics{TurnerFigure1.PNG}
%
%Panel A of Figure 1 from \cite{turner_selective_2008}
%\end{center}
Of course for this sort of exercise to be possible, unless a reader
merely assumes that a registered trial without an associated published
paper produced a null result (as in \cite{rosenthal1979file}), it requires that the registration site
itself obtain outcomes of trials. \href{http://www.clinicaltrials.gov}{ClinicalTrials.gov} is the only
publicly available trial registry that requires such reporting of
results, and only for certain FDA trials.\footnote{\cite{resultscompliance} finds that compliance with results reporting even among those required was fairly low (22\%). HHS and NIH took steps in November 2014 to expand the amount of results reporting required. See \url{http://www.nih.gov/news/health/nov2014/od-19.htm}}  \cite{hartung_reporting_2014} raises
concerns about discrepancies between reporting of outcomes in published
papers and in the \href{http://www.clinicaltrials.gov}{ClinicalTrials.gov} database; as many as 20\% of
studies had discrepancies in primary outcomes and as many as 33\% had
discrepancies in reporting of adverse events, so there is definitely room for improvement. 

Even with dramatic growth in medical trial registration, problems
remain. Not all journals have adopted the ICMJE policy, and complete
enforcement is elusive. \cite{mathieu_s_comparison_2009} looked at trials related
to three medical conditions and found that only 46\% of studies were
registered before the end of the trial with primary outcomes clearly
specified. Even among those adequately registered, 31\% showed some
discrepancies between registered and published outcomes, with bias in
favor of statistically significant definitions.

Almost all registration efforts have thus far been limited to randomized
control trials, as opposed to observational data. Registering all types of analysis should be accepted and encouraged, though there are definitely concerns to registering observational work---not least of which is the inability to verify that registration preceded analysis. See \cite{dal-re_making_2014} for a recent discussion of the pros and cons.
 
\subsection{Social Science Registries}\label{social-science-registries}

Registries in the social sciences are newer but are growing ever more
popular. A brief overview of the major trial registries is shown in Table~\ref{RegTable}, including the total number of studies registered as of April 2018. The Abdul Latif Jameel Poverty Action Lab began hosting a
\href{http://www.povertyactionlab.org/hypothesis-registry}{hypothesis registry}
 in 2009, which was superseded by the American Economic Association's launch of its own \href{http://socialscienceregistry.org}{registry} for randomized trials in May 2013, which had accumulated 260 studies in 59 countries by
October 2014. The International Initiative for Impact Evaluation (3ie)
launched its own registry for evaluations of development programs, the
\href{http://ridie.3ieimpact.org}{Registry for International Development Impact Evaluations} (RIDIE) in September 2013, which had
approximately 30 evaluations registered in its first year.



%\begin{sidewaystable}[]
\begin{table}
\centering
\caption{Registries in Medicine and the Social Sciences}
\label{RegTable}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{llllll}
{\bf Registry} & {\bf Sponsor} & {\bf Year Started} & {\bf Study Design} & {\bf Field} & {\bf Studies Registered} \\
ClinicalTrials.gov & National Institutes of Health & 2000 & RCT & Medicine & 271,000+ \\
International Clinical Trials Registry Platform & World Health Organization & 2007 & RCT & Medicine & 414,000+ \\
AEA RCT Registry (SocialScienceRegistry.org) & American Economic Association & 2013 & RCT & Economics & 1,700+ \\
Registry for International Development Impact Evaluations & 3ie & 2013 & Any & Developing country impact evaluation & 135+ \\
EGAP Design Registry & Experiments in Governance and Politics & 2012 & Any & Political Science & 800+ \\
Open Science Framework & Center for Open Science & 2013 & Any & Any & 18,000+
\end{tabular}
\end{adjustbox}
\end{table}
%\end{sidewaystable}


In political science, \href{http://e-gap.org/design-registration}{EGAP: Experiments in Governance and Politics} has
created a registry as ``an unsupervised stopgap function to store
designs until the creation of a general registry for social science
research. The EGAP registry focuses on designs for experiments and
observational studies in governance and politics.'' EGAP's registry had 93
designs registered as of October 2014.\footnote{Earlier less-widely
  adopted attempts to create registries in political science are the
  Political Science Registered Studies Dataverse (PSRSD,
  \href{../customXml/item1.xml}{http://spia.uga.edu/faculty\_pages/monogan/registration.php})
  and the PAP Registry of the Experimental Research section of the
  American Political Science Association
  (\href{numbering.xml}{http://ps-experiments.ucr.edu/browser}).}

Another location for registrations is the \href{http://osf.io}{Open Science Framework} (OSF), created by the \href{http://centerforopenscience.org/}{Center for Open Science}. The OSF serves as a broad research management tool that encourages and facilitates transparency (see \cite{nosek_scientific_2012}.) Registrations are simply unalterable snapshots of research frozen in time, with a persistent URL and timestamp. Researchers can upload their data, code, hypotheses, etc. to the OSF, register it, and then share the resulting URL as proof of registration. OSF registrations can be relatively free-form, but templates exist to conform to standards in different disciplines. Psychology registrations are presently the most numerous on the OSF \footnote{See \url{https://osf.io/explore/activity/\#newPublicRegistrations}.}

\subsection{Meta-Analysis Research}
Another method of detecting and dealing with publication bias is to conduct meta-analysis. This method of research collects all published findings on a given topic, analyzes the results collectively, and can detect, and attempt to adjust for, publication bias in the literature. A handful of organizations specialize in producing these systematic reviews, including the \href{http://www.cochrane.org}{Cochrane Collaboration} for health studies and the \href{http://www.campbellcollaboration.org/}{Campbell Collaboration} for crime \& justice, education, international development, and social welfare research, and the \href{http://www.3ie.org}{International Initiative for Impact Evalution (3ie)} for social and economic interventions in low- and middle- income countries. The US government supports this type of analysis: the Department of Education's Institute of Education Sciences maintains the \href{http://ies.ed.gov/ncee/wwc/}{What Works Clearinghouse}, and the Department of Labor maintains the \href{http://clear.dol.gov/}{Clearinghouse for Labor and Evaluation Research (CLEAR)}, which serve to collect and grade the evidentiary value of research on education and labor, respectively. (The synthesis methods in the government clearinghouses is not quite as formally statistical in nature as the previously mentioned Collaborations.)

Although quite common in medical research, the tool is not widely used in some parts of the social sciences. But even in economics, where many graduate students are unfamiliar with the technqiue, important papers exist that have quantitatively synthesized bodies of literature, and the \href{https://www.hendrix.edu/maer-network/}{Meta-Analysis in Economics Research Network (MAER-Net)} exists to promote and improve the practice. The unemployment effects of the minimum wage were meta-analyzed in \cite{card1995time}, and the returns to education in \cite{ashenfelter1999review}. A meta-analysis of 87 meta-analyses in economics shows that publication bias is widespread, but not universal. Also see \cite{stanley2005beyond}, which helpfully describes the tools of meta-analysis, and is part of a special issue of \textit{The Journal of Economic Surveys} dedicated to meta-analysis. Sol Hsiang, lead author of a prominent meta-analysis of 60 studies measuring the effect of climate on human conflict \citep{hsiang2013climate}, has also developed the \href{http://dmas.berkeley.edu}{Distributed Meta-Analysis System}, an online tool to crowdsource and simplify meta-analysis.

Prominent examples in psychology include meta-analyses of work on the Big 5 Personality Test and job performance \citep{barrick1991big5} and predictive validity of the Implicit Association Test \citep{greenwald2009understanding}. \cite{simonsohn2014p} also developed a meta-analysis tool that researchers can use to compare the uniform distribution of p-values under the null to the left-skewed distributions observed in research that has been gamed or selectively reported.

%Whoops! This was a duplicate.
%\subsection{Design-Based Review and Publication}
%Another proposed method of reducing publication bias is by altering the time at which peer review takes place. The standard method is for peers to review a completed paper, with results. Even with editorial statements in place encouraging reviewers to not be prejudiced against null results, the odds are stacked against null-result papers. The new method, refered to as `registered reports'\footnote{We consider the name `registered reports' unfortunate, since it can be easily confused with trial registrations, and we prefer the terms `design-based review' or `design-based publication.'} has reviewers evaluate a paper before the data is collected and analyzed. That is, a researcher submits a detailed protocol and analysis plan, and reviewers judge based on the importance of the question being asked and the appropriateness of the methods being used. The protocol is then rejected or given in-principle acceptance. If accepted in-principle, the author conducts the experiment and the analysis as planned, and submits the final result. If the author followed through on his promise, the paper is published, regardless of the statistical significance of the article. 

%This method of review and publication is championed by Chris Chambers at Cardiff University. Nearly 20 journals have adopted this method to one degree or another. More information can be found at the project's \href{https://osf.io/8mpji/wiki/home/}{Open Science Framework page}. It is too early to do a sophisticated analysis of the effects on publication, but we believe it to be a promising development. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Researcher Degrees of
Freedom}\label{rdof}

Though registration helps solve the problem of publication bias, it does
not solve the problem of fishing for statistical significance within a
given study. This problem with research is known as data mining: the manipulation or repeated searching through statistical or regression
models unknowingly (or deliberately) until significance is obtained.
 \cite{simmons_false-positive_2011} refer to this as
``researcher degrees of freedom,'' and it has also been referred to as
``fishing,'' ``p-hacking,'' or ``specification searching'' \citep{humphreys_fishing_2013}. The problem has many names because it can take many shapes. Using flexibility around when to stop
collecting data, excluding certain observations, combining and comparing
certain conditions, including certain control variables, and combining
or transforming certain measures, they ``prove'' that listening to the
Beatles' song ``When I'm Sixty-Four'' made listeners a year and a half
younger. The extent and ease of this ``fishing'' is also described in
\cite{humphreys_fishing_2013} who use simulations to show that multiplicity of outcome measures, multiplicity of heterogeneous treatment effects (sub-group analyses), and multiplicity of cut-points for turning a continuous outcome variable into a binary outcome, can all be used to virtually guarantee a false positive, even with large sample sizes. They also find that selective adding of covariates can produce false positives with small samples, though they do find little room to produce false positives through arbitrary selection of model for binary outcomes (linear, logit, or probit) regardless of sample size. \cite{gelman_garden_2013} agree that
``{[}a{]} dataset can be analyzed in so many different ways (with the
choices being not just what statistical test to perform but also
decisions on what data to exclude or exclude {[}sic{]}, what measures to
study, what interactions to consider, etc.), that very little
information is provided by the statement that a study came up with a
\emph{p}\textless{}.05 result.'' However, they also conclude that:

\begin{quote}
the term ``fishing" was unfortunate, in that it invokes an image of a
researcher trying out comparison after comparison, throwing the line
into the lake repeatedly until a fish is snagged. We have no reason to
think that researchers regularly do that. We think the real story is
that researchers can perform a reasonable analysis given their
assumptions and their data, but had the data turned out differently,
they could have done other analyses that were just as reasonable in
those circumstances.

We regret the spread of the terms ``fishing" and ``p-hacking'' (and even
``researcher degrees of freedom'') for two reasons: first, because when
such terms are used to describe a study, there is the misleading
implication that researchers were consciously trying out many different
analyses on a single data set; and, second, because it can lead
researchers who know they did not tryout many different analyses to
mistakenly think they are not so strongly subject to problems of
researcher degrees of freedom."
\end{quote}

In other words, the problem may be even worse than you think. What can be
done to solve it? Part of the answer lies in detailed
pre-analysis plans, described below.

\subsection{Pre-Analysis
Plans}\label{pre-analysis-plans}

While registration of studies can help to reduce publication bias or the file drawer problem, 
A pre-analysis plan (PAP), a detailed outline of the analyses that will be conducted in a study, can be used to reduce researcher degrees of freedom. Registration is now the norm in medicine for randomized trials, and these often include (or link to) prospective statistical analysis plans as part of the project protocol. Official guidance from the US Food and Drug Administration's Center for Drug Evaluation and Research (CDER) from 1998 describes what should be included in a statistical analysis plan and discusses eight items related to data analysis that should be considered: pre-specification of the analysis; analysis sets; missing values and outliers; data transformation; estimation, confidence intervals, and hypothesis testings; adjustment of significance and confidence levels; subgroups, interactions, and covariates; and integrity of data and computer software validity \citep{food1998guidance}.   %Sometimes this is because the medical researcher intends to do very little, if any, structural or formal modeling. But even if a researcher intends only to compare unadjusted means and bootstrap for standard errors, this should be explicitly stated. In the social sciences, this simple comparison is often not the end goal of a randomized trial, so there may be even more value in pre-specification of intended analyses.
This is an excellent start, and in the section below I discuss adapting these ideas for a pre-analysis plan in the social sciences.

A pre-analysis plan (PAP) contains a specification of the outcomes of the study (sometimes referred to as endpoints in the medical literature), as well as a specification of the methods that will be used to analyze the outcomes. By describing the
method(s) of analysis ahead of time, and to some degree tying the hands
of the researcher, we reduce the ability to data mine. Though one
example of this exists in economics from 2001 \citep{neumark_employment_2001}, the idea
is still quite new to the social sciences. The level of detail varies
widely, and the research community is still constructing norms for
incorporating these documents into final analyses and papers.

\paragraph{What to Include}
Suggestions have been made for the detailed contents of these documents.
\cite{glennerster_running_2013} suggest including the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  the main outcome measures,
\item
  which outcome measures are primary and which are secondary,
\item
  the precise composition of any families that will be used for mean
  effects analysis,
\item
  the subgroups that will be analyzed,
\item
  the direction of expected impact if we want to use a one-sided test,
  and
\item
  the primary specification to be used for the analysis.
\end{enumerate}
David McKenzie of the World Bank Research Group proposed a list of ten
items that should be included in a PAP, reproduced below from the \href{http://blogs.worldbank.org/impactevaluations/a-pre-analysis-plan-checklist}{World Bank Development Impact Blog}.

\begin{enumerate}
\item
  Description of the sample to be used in the study
\item
  Key data sources
\item
  Hypotheses to be tested throughout the causal chain
\item
  Specify how variables will be constructed
\item
  Specify the treatment effect equation to be estimated
\item
  What is the plan for how to deal with multiple outcomes and multiple
  hypothesis testing?
\item
  Procedures to be used for addressing survey attrition
\item
  How will the study deal with outcomes with limited variation?
\item
  If you are going to be testing a model, include the model
\item
  Remember to archive it
\end{enumerate}

In their article on researcher degrees of freedom, Simmons, Nelson, and
Simonsohn (2011) suggest the following requirements for authors:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Authors must decide the rule for terminating data collection before
  data collection begins and report this rule in the article.
\item
  Authors must collect at least 20 observations per cell or else provide
  a compelling cost-of-data-collection justification.
\item
  Authors must list all variables collected in a study.
\item
  Authors must report all experimental conditions, including failed
  manipulations.
\item
  If observations are eliminated, authors must also report what the
  statistical results are if those observations are included.
\item
  If an analysis includes a covariate, authors must report the
  statistical results of the analysis without the covariate.
\end{enumerate}

\paragraph{Expecting the Unexpected}
\cite{glennerster_running_2013} also mention the ``tension between
the benefits of the credibility that comes from tying ones hands versus
the benefits of flexibility to respond to unforeseen events and
results.'' Writing a PAP can lend extra credibility to research by making it of a confirmatory nature as opposed to an exploratory nature.
Both types of research are absolutely valuable, but knowing the distinction is important. If some sort of restriction on the data, be it a specific functional form, exclusion of outliers, or an interaction term (subgroup analysis) that turns a null effect for the population into a significant effect for some subgroup, is specified ahead of time based on theory or previous research, this can be considered confirmatory research. 

Some would say this is of more value than the exploratory research approach of simply running 20 sub-group analyses and finding that one or two are significant. This may be an estimate of a true effect, but should be labeled as exploratory, and future researchers could attempt to confirm this finding by addressing the question of the sub-group specifically. The potential downside to pre-stating hypotheses and analysis plans is that no matter how carefully researchers plan ahead, something truly unexpected can occur. (An example discussed at a recent conference was subjects showing up for an experiment stoned. One example from a field experiment involved fatalities from a lightning strike at a school that was part of a competitive girls scholarship program \citep{kremer2009incentives}. ) This is why, even though I may use the phrase ``bind our hands,'' I advocate that researchers not be punished for conducting research outside the analysis plan. I simply recommend that researchers clearly delineate which analysis was included in the analysis plan, and which was not, so that readers can know what is confirmatory and what is exploratory.


\paragraph{When to Write}
There is some question as to when one should write one's pre-analysis plan. ``Before you begin to analyze your data'' seems like the obvious answer, but this should be precisely defined. One could write the PAP before any baseline survey takes place, after any intervention but before endline, or after endline but before analysis has begun. \cite{glennerster_running_2013} and \cite{OlkenPAP} have an informative discussion of the relative values of PAP timing. If one writes the PAP before the baseline, this is in some sense the purest, most free from accusations of p-hacking, but one could also miss valuable information. For example, suppose in baseline one learns that the intended outcome question is phrased poorly and elicits high rates of non-response, or that there is very little variation in the answers to a survey question. If the PAP was written after baseline, one could have accounted for this, but at the same time, researchers would also be free to change the scope of their analysis--for example, in the baseline survey of a field experiment designed to increase wages revealed that few of the subjects worked outside the home, the researcher could change the focus of the analysis. This is not necessarily wrong, but it does change the nature of the analysis somewhat.

PAPs could also be written after endline data has been collected but before the investigators have begun to analyze the data. Some have suggested that one could even look at baseline data from the control group only before writing the PAP. I find this problematic, however, since a researcher could learn that the control group had a particularly low or high value of a certain outcome variable, and then choose to include or not include this variable in the analysis as a result. The original research design could have been intended to analyze the increase in secondary school attendance, but looking at the control group, the researcher sees that the control group had a very high rate of attendance, making a significant difference between control and treatment (the treatment effect) unlikely. Learning this after the experiment has concluded and searching for things that might be easily different between treatment and control is more exploratory than confirmatory. An alternative proposal discussed in \cite{OlkenPAP} is to remove the treatment status variable from the dataset before looking at the data, which seems to alleviate some of the concerns. However, one could still search for sub-group analyses at this stage. If you parse the outcome data by gender, and males and females have a similar distribution, to find a differential treatment effect by gender would seem unlikely. If male and female had wildly different outcomes, it would seem like a significant interaction is more likely. This is more exploratory than confirmatory. 


\subsubsection{Examples}\label{examples}
Examples of pre-analysis plans in the social sciences are relatively rare, but several examples of good published papers resulting from studies with PAP exist. Several of the items below come from the \href{http://www.povertyactionlab.org/Hypothesis-Registry}{J-PAL Hypothesis Registry}; I highlight those that have publicly available final papers. 


\begin{itemize}
\item 
 \cite{casey_reshaping_2012} includes evidence from a large-scale field experiment on community driven development projects in Sierra Leone. The analysis finds no significant benefits. Given the somewhat vague nature of the development projects that resulted from the funding, and the wide variety of potential outcomes, finding signficant results would have been relatively easy. In fact, the paper includes an example of how, if they had the latitude to define outcomes without a pre-analysis plan, the authors could have reported either large and significantly positive or negative outcomes, depending on their preferences. The paper also includes a discussion of the history and purpose of pre-analysis plans. The \href{http://emiguel.econ.berkeley.edu/assets/miguel_research/8/_Appendix__Reshaping_Institutions_-_Evidence__on__Aid__Impacts__Using__a__Pre___Analysis__Plan.pdf}{online appendix}
contains the PAP.

\item 
Oregon expanded its Medicare enrollment through a random lottery in 2008, providing researchers with an ideal avenue to evaluate the benefits of enrollment. \cite{finkelstein_oregon_2012, doi:10.1056/NEJMsa1212321} and \cite{Taubman17012014} show that recipients did not improve in physical health measurements, but were more likely to have insurance, had better self-reported health outcomes, utilized emergency rooms more, and had better detection and management of diabetes. Pre-analsyis plans from the project are available at the \href{http://www.nber.org/oregon/documents.html}{National Bureau of Economics}' site devoted to the project. (See, for example, \cite{taubman_oregon_2013, baicker_katherine_oregon_2014}.)


\item
The shoe company Toms funded a rigorous evaluation of its in-kind shoe donation program. Researchers wrote a pre-analysis plan before conducting their research, and found no evidence that shoe donations displace local purchasing of shoes. See \cite{wydick_-kind_2014, katz_elizabeth_pre-analysis_2013}. The PAP is available in the \href{http://www.povertyactionlab.org/doc/pre-analysis-planwydick2-12-13pdf}{JPAL Hypothesis Registry}. This is one of many projects that has benefited from a pre-analysis plan because of the involvement of a group with a vested interest, such as a government or corporation. Even researchers skeptical of the need for PAPs in general admit the benefit to publicly pre-stating analysis plans when someone involved has such a clear incentive for results to go a certain direction. 

\item
Researchers from UC San Diego and the World Bank evaluated job training programs run by the Turkish government and found only insignificant improvements and a strongly negative return on investment. See \cite{almeida_impact_2012, vocationalTurkey}. The PAP is available in the \href{http://www.povertyactionlab.org/Hypothesis-Registry}{J-PAL Hypothesis Registry} as well as the \href{http://blogs.worldbank.org/impactevaluations/files/impactevaluations/iskurie_analysisplan_v4a.pdf}{World Bank Development Impact Blog}.



\item 
Teams led by Ben Olken have evaluated multiple randomized interventions in Indonesia. The \href{http://www.povertyactionlab.org/evaluation/project-generasi-conditional-community-block-grants-indonesia}{Generasi program} linked community block grants to performance. The PAP are \cite{olken_generasi_2009, olken_generasi_2010} and are available in the \href{http://www.povertyactionlab.org/Hypothesis-Registry}{J-PAL Hypothesis Registry}. The researchers found health improvement, but no education improvement \citep{olken_indonesias_2010, olken_should_2014}.

\item 
Another project in Indonesia used a field experiment to evaluate different means of poverty targeting for cash transfer programs: proxy-means testing, community-based targeting, and a hybrid of the two. Results show that the proxy-means testing outperformed the other methods by 10\%, but that community members were far more satisfied with the community method. The PAP and final paper are available as \cite{olken_targeting_2009} and \cite{alatas_targeting_2012}

\item
An example from pyschology is a pre-registered replication of an implicit association test. Existing research showed evidence of stronger racial preferences among fertile women. \cite{NosekPreRegistered} failed to reproduce this effect in four tries, suggesting the association is weaker than originally found. The resulting manuscript, as well as the time-stamped registration of the analysis plan, can be found on the Open Science Framework at \url{https://osf.io/g3sca/}.
\end{itemize}
Additionally, Alejandro Ganimian developed \href{http://scholar.harvard.edu/files/alejandro_ganimian/files/pre-analysis_plan_template_0.pdf}{a template for pre-analysis plans} that instructors may find useful when teaching tranpsarency methods, or researcher themselves may find useful when developing their own pre-analysis plan.

\subsubsection{Observational Pre-Analysis Plans}
As with registration, part of the concern with pre-analysis plans is whether the ``pre'' aspect is verifiable. How can we be sure that a researcher didn't look at observational data, run a few hypothesis test, and then write the PAP? One way is to use reliably known availability dates of certain datasets such as government adminstrative data. Verifiable pre-specification of observational research was undertaken in \cite{neumark_employment_2001}, which prospectively detailed analysis of the unemployment effects of the minimum wage, a question that has been debated for at least a century \citep{Neumark_2014_bathwater}.\footnote{An editorial introduction to \cite{neumark_employment_2001} was accidentally left out of the issue, and published the following month, see \cite{Levine2001editorial}. Others followed Neumark's paper in spirit by using the same regression specifications on data from Canada \citep{CanadianMinWage2006}.} 

The federal minimum wage changed in October 1996 and September 1997. Neumark submitted a pre-specified research design consisting of the exact estimating equations, variable definitions, and subgroups that would be used to analyze the effect of the minimum wage on unemployment of younger workers using October, November, and December CPS data from 1995 through 1998. This detailed plan was submitted to journal editors and reviewers prior to the end of May 1997; the October 1996 data started to become available at the end of May 1997, and Neumark assures readers he had not looked at any published data at the state level prior to submitting his analysis plan. 

Neumark's hope was to eliminate the potential for bias due to ``author effects.'' The obvious time stamp of the federal governmentâ€™s release of data indeed makes this possible, but the situation also benefits from the depth and intensity of the debate prior to this study. Neumark had an extensive extant literature to rely on when picking specific functional forms and subgroup analyses to be conducted. He tests two definitions of the minimum wage, the ratio of the minimum wage to the average wage (common in Neumark's previous work), as well as the fraction of workers who benefit from the newly raised minimum wage (used in David Card's earlier work \citep{Card1992minwage}) and tests both models with and without controls for the employment rate of higher-skilled prime age adults (as recommended by \cite{Deere1995minwage}). 

These equations are also each estimated with only a lagged coefficient for minimum wage, and only for the outgoing rotation group (ORG) subset of the data. The equations are estimated for 16-19 year olds, 16-24 year olds, 16-24 year olds with no more than high school education, 20-24 year olds with no more than high school education, and 16-20 and 20-24 year olds (separately) with less than high school education. The results show mostly insignificant results, but that 18 of the 80 specifications result in statistically significant decreases in employment (at the .10 or .05 level), with elasticities ranging from -0.14 to -0.3 for significant estimates and smaller (closer to zero) for insignificant estimates. 

How widely could Neumark's method of pre-specification prior to data-release become, and what do we gain from it? Thanks to pre-specification we know exactly how many hypothesis tests Neumark ran (at least 80 coefficients, and more with joint tests), so we know the context in which to appropriately interpret the p-values. This is a significant improvement over much other observational research. Of course, econometricians could still argue about the appropriateness of certain models (whether to include lags or look at contemporaneous effects, for instance) and the analysis suffers from noisy estimates given the relatively small number of changes to state minimum wages compared to other time periods that had been previously analyzed using similar methods.  

It is also difficult to see how a researcher could reach this level of detail with a research question with which they were not already intimately familiar; it seems more likely that one would either pre-specify with either a completely inadequate level of detail, or a wildly inappropriate specification. But perhaps this familiarity is a good thing. Much exploratory observational research could continue as is, and for important and thoroughly debated and defined question, confirmatory research could be conducted in a pre-specified fashion. Although pre-specification is difficult, given the vast troves of data released on reliable schedules by governments of developed countries through the world, we are certain that the number of cases is which verifiable pre-specification of observational research could and should be conducted is far greater than the number of times it has been conducted.

\subsubsection{Project Protocols}\label{project-protocols}

A project protocol can be somewhat similar to a PAP, but is distinct. A protocol is a detailed recipe or instruction manual for others to use to reproduce an experiment. Protocols are important both in helping solve researcher degrees of freedom problems by making the exact details of analysis known and help avoid selective reporting, as well as in making one's work reproducible. Protocols are standard in the medical literature, as in areas of lab science, but may be less familiar to those used to working with administrative or observational data. Lab sciences are rife with examples of experiments failing to replicate because of supposedly minor changes such as the brand of bedding in mouse cages, the gender of the laboratory assistant, or the speed at which one stirs a reagent \citep{sorge2014olfactory, hines2014sorting}, and we assume the same situation exists in the social sciences. \textit{Nature} has decided to expand its methods section in order to encourage better reporting.\footnote{\url{http://www.nature.com/news/announcement-reducing-our-irreproducibility-1.12852}}

The social sciences would benefit from more careful documentation of methods. When one uses administrative data this can be accomplished by sharing one's data and code so that analysis is transparent.\footnote{It should be noted that the need for documentation of survey method is not eliminated by using administrative data, the burden simply falls upon the administration.} This is discussed below in section~\ref{replication-and-reproducibility}. With original data collection, researchers should provide very detailed descriptions of what exactly they did. A 33-item checklist of suggested items is contained in the SPIRIT(\href{http://www.spirit-statement.org}{Standard Protocol Items: Recommendations for Interventional
Trials}) statement \citep{chan_spirit_2013}, including details on the participants, interventions, outcomes, assignment, blinding, data collection, data management, and statistical methods, among other things. 

One area in which social sciences could improve involves details of randomization. \cite{bruhn_pursuit_2009} documents the lack of clear explanation pertaining to how randomization was conducted in RCTs published in economics journals. Variables used for stratification are not described, and the decision of whether to control for baseline characteristics was often done after the fact. While there seems to be internal disagreement in both
medicine and the social sciences over the appropriateness of including baseline
control variables in regression analysis of a randomized trial, having
researchers selectively report whatever method gives them the most
significant-seeming results is obviously not the optimal outcome. 

The medical literature also exhibits much greater concern over the blinding and concealment of randomized assignment than some of the social science literature. In some situations, blinding is impossible or irrelevant in a social science field experiment: for example, the recipient of a cash transfer needs to know that they received cash in order for the program to have any effect. Also, a social scientist interested in the potential scaling up of a government program may rightfully be unperturbed by some respondents assigned to the control group somehow gaining access to treatment, since this behavior would undoubtedly occur if the program were scaled up and the researcher still has a valid intention to treat estimate.  This is clearly not always the case, especially if one wants an accurate estimate of the efficacy of a program or a treatment on the treated estimate. Tales of trials ruined through carelessness with the original randomization assignment as well as tips on how to avoid the same problem are described in \cite{schulz_allocation_2002}. In addition to \cite{bruhn_pursuit_2009}, political science has produced guidelines for randomization and related disclosure, avaialable at \url{http://e-gap.org/resources/guides/randomization/}.


 Some medicine and science journals have begun to publish protocols.   While the
advantages of publishing a protocol related to the development of a new
procedure (e.g. ``we have developed a new method of isolating mRNA'')
should be obvious, the advantages of publishing protocols for randomized
trials under way are perhaps less obvious, but still exist. \emph{BioMed
Central} and \emph{BMJ Open}, among others, now publish protocols of
trials planned or ongoing, with the hopes that this will reduce
publication bias, allow patients to see trials in which they might like
to enroll, allow funders and researchers to learn of work underway to
avoid duplication, and to allow readers to compare what research was
originally proposed to what was actually completed.\footnote{See
\url{http://www.biomedcentral.com/authors/protocols} and
\url{http://bmjopen.bmj.com/site/about/guidelines.xhtml\#studyprotocols}.}
\emph{BMJ Open} suggests, but does not require, that its published
protocols include the items in the SPIRIT checklist.

Protocols are not a perfects solution, as even in published (or otherwise public) protocols, studies have found important differences between protocols and published results. 60-71\%
of outcomes described in protocols went unreported in the paper while
62\% had major discrepancies between primary outcomes in the protocols
and in the published papers, though there was a relatively even mix of
these discrepancies favoring significant or insignificant results \citep{chan_a_empirical_2004}. Another study found that appropriate level of statistical detail is often lacking in protocols, and there are often discrepancies between protocols and published results \citep{saquib_practices_2013}. 31\% of published papers had some sort of pre-specified plan for their regression adjustments (i.e.~specifying which baseline covariates would be controlled for), %while 70-74\% of those that published a design paper or provided a protocol to the authors, 
but only 53\% of the plans matched what was published in the ultimate paper.


\subsection{Multiple Hypothesis Testing}
Several of the PAP and lists of suggestions above include corrections for multiple hypothesis testing. The idea of correcting for multiple tests is widespread in certain fields, but has yet to take hold in the social sciences. Simply put, the idea is that because we are aware of the fact that test statistics and p-values appear significant purely by chance a certain proportion of the time, we can report different, \textit{better} p-values that control for the fact that we are running multiple tests. There are several ways to do this, a few of which are used and explained in a simple and straightforward manner by \cite{anderson_fwer}:
\begin{itemize}
\item
Report index tests---instead of reporting the outcomes of numerous tests, standardize outcomes and combine them into a smaller number of indeces (e.g. instead of separately reporting whether a long-term health intervention reduced blood pressure, diabetes, obesity, cancer, heart disease, and Alzheimer's, report the results of a single health index.) \cite{kling2007experimental} implements an index test from the Moving to Opportunity field experiment, using methods developed in biomedicine by \cite{obrien1984procedures}.

\item
Control the Family-Wise Error Rate (FWER)---FWER is the probability that at least one true hypothesis in a group is rejected (a type I error), meaning it is advisable when the damage from incorrectly claiming \textit{any} hypotheses are false is important. There are several ways to do this, with the simplest (but very conservative) method being the Bonferroni correction of simply multiplying every original p-value by the number of tests done. Holm's sequential method involves ordering p-values by class and multiplying the lower p-values by higher dicsount factors \citep{holm_multipletesting}. An efficient recent method is the free step-down resampling method, developed by \cite{westfall_young_multiple}.
  
\item
Control the False Discovery Rate (FDR)---In situations where a single type I error is not catastrophic, researchers may be willing to use a less conservative method and trade off some incorrect rejections in exchange for greater power. This is possible by controlling the FDR, or the percentage of rejections that are type I errors. \cite{benjamini1995controlling} details a simple algorithm to control this rate at a chosen level, and \cite{benjamini2006adaptive} describes a two-step procedure with greater power. 


\end{itemize}

\subsection{Subgroup Analysis}
One aspect of researcher degrees of freedom related to multiple hypothesis testing that seems to have taken hold widely in the medical literature is the aversion to sub-group analysis (``interactions'' to
most economists). Given the ability to test for a differential effect by many different groupings, crossed with each outcome variable, sub-groups analysis can almost always find some sort of supposedly significant effect. An oft-repeated story in the medical literature revolves around the publication
of a study on aspirin after heart attacks. When the editors suggested including 40 subgroup analyses, the authors relented on the condition
they include some of their own. Gemini and Libras had worse outcomes
when taking aspirin after heart attacks, despite the large beneficial
effects for everyone else. (Described in \cite{schulz_multiplicity_2005},
with the original finding in \cite{isis-2_second_international_study_of_infarct_survival_collaborative_group_randomised_1988}) Whether in a randomized trial or
not, I feel that social scientists could benefit from reporting the number of
interactions tested, possibly adjusting for multiple hypotheses, and
ideally specifying beforehand the interactions to be tested, with a justification from theory or previous evidence as to why the test is of interest. 


\subsection{Results-Blind Reviewing}
A new development in research transparency which helps to address both publication bias and researcher degrees of freedom is results-blind reviewing. In results-blind reviewing, authors submit a detailed research plan \textit{before} conducting the research. They submit this plan to a journal, and the journal rejects or gives an in-principle acceptance of the not-yet-written article based on the scientific merit of the question being asked and the methods proposed to answer them, as opposed to whether the results pass an arbitrary threshold of statistical significance. Then the authors conduct the research, and their paper is published as long as they don't deviate too much from what they initially proposed. The editors and reviewers have tied their hands and have no ability to accept only significant results, and the authors have less incentive to game their statistical analysis in order to find something signficant. 

This new mode of publication, called ``Registered Reports," is championed by Chris Chambers, psychologist at Cardiff University, and has been adopted by over one hundred journals. See a full list of journals adopting the procedure at \url{https://cos.io/rr/}. \textit{Social Pyschology} ran an issue dedicated to this type of article, with an editorial explaining the concept \citep{nosek2014registered}. \textit{AIMS Neuroscience}, which uses this format, published an editorial answering 25 frequently asked questions pertaining to registered reports \citep{chambers2014instead}. In economics, the first journal to pilot this practice is the \textit{Journal of Development Economics}, announced in March 2018 on the \href{https://blogs.worldbank.org/impactevaluations/registered-reports-piloting-pre-results-review-process-journal-development-economics}{World Bank Development Impact blog}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Replication and
Reproducibility}\label{replication-and-reproducibility}

\begin{quote}
``Economists treat replication the way teenagers treat chastity - as an
ideal to be professed but not to be practised.''---Daniel Hamermesh,
University of Texas at Austin Economics
\end{quote}

\begin{quote}
``Reproducibility is just collaboration with people you don't know,
including yourself next week''---Philip Stark, University of California Berkeley Statistics
\end{quote}

Replication, in both practice and principle, is a key part of social
science research. I first define what exactly I mean by replication
using the taxonomy developed in \cite{hamermesh_viewpoint:_2007} and \cite{hunter_desperate_2001}.
According to Hamermesh, replication comes in a few different shapes: pure, statistical, and
scientific.

\begin{itemize}
\item
  Pure: Using the exact same data and the exact same model to see if the
  published results are reproduced exactly.
\item
  Scientific: Using a different sample from a different population, and
  similar, but perhaps not identical model .
\item
  Statistical: Using the same model and underlying population but a
  different sample. In Hamermesh's view, this is less relevant to certain fields, such as economics,
  where researchers are likely to already use as large a sample as is available.
\end{itemize}

One might imagine a fourth type that uses the same data but probes the data using additional robustness and sensitivity checks. Others have described replication in terms of a spectrum from full replication (independent collection of data and re-running analysis) to
reproducibility, where the same data and code are re-used by other
researchers \citep{peng_reproducible_2011}. Whatever the terminology used, replication or reproducibility, transparent research requires making data and code available to other researchers so they can try and get the same results.


\subsection{Code and Workflow}\label{code-and-workflow}

Reproducing research often involves using the exact code and statistical
programming done by the original researcher. To make this possible, code
needs to be both (1) easily available and (2) easily interpretable.
Thanks to several free and easy to use websites described below, code
can easily be made available by researchers without requiring funding or
website hosting. Making code easily interpretable is a somewhat more complicated
task, nevertheless, the extra effort spent to make a more manageable
code pays off with large dividends.

\subsubsection{Publicly Sharing Code}\label{publicly-sharing-code}

Once analysis is complete (or even before this stage) researchers should
share their data and code with the public. GitHub
(\url{http://www.github.com}), The Center for Open Science's Open
Science Framework (\url{http://osf.io}), and Harvard University's
Dataverse (\url{http://thedata.org}) are all free repositories for data
and code that include easy to use version control.\footnote{BitBucket
  (\href{styles.xml}{http://www.bitbucket.org}) is another
  web service that one can use for free version control and archiving
  of public data and code.} 
  
Version control is a powerful way to archive versions of files so that old versions are not lost and can be returned
to if needed. The most naive way to organize a project would be to just name a file ``MyAnalysis.do'' and then save over it any time you or your coauthor made changes. I sincerely hope no one is working this way, since you are losing tons of information. Most people are probably doing some version of the ``date and initials'' method. Instead of simply calling one's analysis code ``MyAnalysis.do'' and repeatedly saving over and losing old versions, when you save a new version, you add the date to the file name: ``MyAnalysis.2014.08.13.do'', and if a co-author makes changes on the same day, they add their initials:``MyAnalysis.2014.08.13\_GC.do'' or they change the date: ``MyAnalysis.2014.08.14.do''. This can work on small projects, but it really isn't the best way to work. Why does the operating system tell me the file from August 13 was modified in October? What if one analysis script file is called 14 times in 9 different files---did you remember to go through and update all the calls? \cite{GentzkowShapiro} provide strong evidence of the inadequacy of dating and initialing : ``\textit{Not one piece of commercial software you have on your PC, your phone, your tablet, your car, or any other modern computing device was written with the ``date and initial'' method.}'' [emphasis original]

The solution is version control (also referred to as revision control). Version control is free software (\href{https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control}{Git} and \href{https://mercurial.selenic.com/}{Mercurial} are popular and free implementations) that stores all versions of files you create, and can easily compare versions of the files to highlight changes, revert to earlier versions, accept or reject changes proposed by a collaborator, and reconcile conflicting versions when different people make changes to the same file. Version control creates a repository, and stores all versions of your files in the repository\footnote{Repositories can take up a significant amount of space, but you can avoid this problem by not storing generated files or binaries (.pdf, .docx, .etc) since every user with access to the repository should be able to recompile and generate files on their own. Version control works best with simple text files.}  Web services
such as \href{http:www.github.com}{GitHub} offer free hosting of these repositories with easy to use web-based interfaces and GUI software, so that you and your collaborators can all access the same files, but you can easily host the repository on a server of your own. 

\subsubsection{Managing Workflow}\label{managing-workflow}

Code is just one aspect of a larger structure I refer to as
``workflow'' after \cite{long_workflow_2008}, by which I mean the
combination of data, code, organization, and documentation: everything
from file and variable names to folder organization as well as efficient
and readable programming, and data storage and documentation. I strongly recommend that Stata-users read \cite{long_workflow_2008} and R users read \cite{gandrud2013reproducible} for workflow recommendations both general and specific to their respective programming language\footnote{Also see and \cite{kirchkamp_workflow_????}}. Our suggestions here borrow heavily from their excellent work. 

Another excellent, more brief work is Matthew Gentzkow and Jesse Shapiro's excellent manual on code and data \href{https://www.brown.edu/Research/Shapiro/pdfs/CodeAndData.pdf}{\citep{GentzkowShapiro}}. They come from the same background as us--they didn't take many programming classes, they're just clever and figured it out. But once their data got massive, and their programming problems got massive with it, they figured they should listen to the fulltime programmers and database managers who had spent years and billions of dollars solving these problems. Their manual adapts many of these solutions to data-based social science research.   

I also refer undergraduate instructors and others who may be interested to Richard Ball and Norm Medeiros' \href{http://www.haverford.edu/TIER/}{Project TIER} (Teaching Integrity in Empirical Research), which is a ``protocol for comprehensively documenting all the steps of data management and analysis that go into an empirical research paper.'' A specific file organization using the Open Science Framework is taught so that teachers can exactly reproduce the work of every student (and so students can reliably get the same answer every time they conduct their analysis). 

\paragraph{Software:}

Although I agree with the movement by many towards open source software such
as \href{http://www.r-project.org/}{R} and \href{https://www.python.org/}{Python}, I appreciate that many disciplines have long
traditions of using proprietary software such as SAS and STATA, and
learning a new programming language may be an undesirable additional
task in researchers' busy lives. That said, there are several general
coding rules that all researchers should use when organizing and
implementing their analysis, and researchers should strive to make their
work usable by as many others as possible.

\paragraph{Writing Code:}
Perhaps the most important rule is to write code instead of working by hand. By
that I mean:

\begin{itemize}
\item
  Do not modify data by hand, such as with a spreadsheet. Which is to
  say, don't use Excel.
\item
  Use neither the command line nor drop-down menus nor point-and-click
  options in statistical software.
\item
  Instead, do everything with scripts.
\end{itemize}

The simple reason for this is reproducibility. Modifying data in Excel
or any similar spreadsheet program leaves no record of the changes made
to the data, nor any explanation of the reasoning or timing behind any
changes. Although it may seem easy or quick to do a one-time-only
cleaning of data in Excel, or make ``minor'' changes to get the data
into a format readable by a researcher's preferred statistical software,
unless these changes are written down in excruciating detail, this is
not reproducible by other researchers. It is better to write a programming
script that imports the raw data, does all necessary changes, with
comments in the code that explain changes, and saves any intermediate
data sets used in analysis. Then, researchers can share their initial
raw data and their code, and other researchers can reproduce their work
exactly.

Though I understand that a fair amount of research has been done using
pull down menus in SPSS or Stata, I advise against this. A bare minimum
if one insists on going this route is to use the built-in
command-logging features of the software. In Stata, this involves the
`cmdlog' command, in SPSS, this involves the paste button to add to a
syntax.

The ideal is to make everything, including changes like rounding and
formatting, done with scripts. Even downloading of data from websites
can be done through a script. For example, in R, the download.file()
function can be used to save data from a website. (Though of course this
opens the possibility to the data file changing. When reproducing
results from a given dataset is more important than the data from a
specific source, researchers should download their raw dataset once, and
never save over it, instead saving all modified intermediate datasets in
a separate location.) Another extremely important way to prevent
unintentional changes to data is to always set the seed for random
number generators whenever any random numbers are to be used (set.seed()
in R, set seed () in Stata). Additionally, information about the exact
software version used should be included (include the `version' command in Stata, or use the
session.info() command in R) as well as computer processor and operating
system information. The casual programmer may assume that sophisticated
software would always produce the exact same answer across multiple
versions of software and platforms, but this is not the case. This is also definitely not the case with user-written packages. R users can use the packageVersion() command, and can run old versions of packages since they are archived at \href{http://cran.r-project.org}{CRAN}. Stata users can use the viewsource command for any .ado they use, but since the Statistical Software Components (SSC) unfortunately does not archive old versions, reproducibility may be lost, so ideally researchers would include the actual code for the version of the user-written .ado along with their publicly archived data and code files


Finally, two simple organizing principles to consider are:
\begin{enumerate}
\item 
 Consider not saving statistical output, and just saving the code and data that generates it. Obviously this would be unrealistically time consuming for large projects, but the idea is that you should be able to reproduce all steps of your analysis that you could in theory take this approach.

\item
What would happen if you, or your laptop hard drive, were hit by a bus? How easily would anyone else be able to reproduce your work? Hopefully the probability is non-zero.
\end{enumerate}

\subsection{General Workflow
Suggestions:}\label{general-workflow-suggestions}
Here I offer some specific workflow organization suggestions that should be valid regardless of code or operating system.
\begin{itemize}
\item
  Do not use spaces in directory or file names, as it complicates referring to them in certain software.
\item
  Add name, date, and describe contents, as well as updates, to all
  scripting files.
\item
  Keep a daily research log, i.e. a detailed written diary of what research is done on a given day. You'll be surprised how often this will be useful to answer questions about whether you ran a certain test or not, when you did it, and what you called the file.
  
\item
    Make sure that your script files are self-contained. That is, don't write a program that only works if you run a group of other files previously in a specific order and then leave things a certain precarious way. You should be able to run intermediate steps of the workflow without disrupting things downstream.
 
\item
  You can never comment too much.
\item
  Indent your code
\item
    Once you post/distribute code or data, any changes at all require a new file name.
\item
  Separate your cleaning and analysis files; don't make any new variables
  that need saving (or will be used by multiple analysis
  files) in an analysis file---it is better to only create the variables once so you know they're the identical when used in different analysis files.
\item
  Never name a file ``final'' because it won't be.
\item
  Name binary variables ``male'' instead of ``gender'' so that the name is more informative. 
\item
  Use a prefix such as x\_ or temp\_ so you know which files can easily
  be deleted.
\item
  Never change the contents of a variable unless you give it a new name.
\item
  Every variable should have a label.
\end{itemize}

\subsection{Stata-specific
Suggestions}\label{stata-specific-suggestions}

\begin{itemize}
\item
  Use the full set of missing values available to you (``.a''-``.z'', not exclusively
  ``.'') in order to distinguish between ``don't know'' and ``didn't ask'' or other distinct reasons for missing data.
\item
  Make sure code always produces same result---if you use anything randomly generated, set the seed. When sorting or merging, you need to be sure to uniquely specify observations, because if you don't, Stata does something arbitrary and not repeatable. So instead of just sorting or merging on `ID' when there are multiple observations per ID, sort by `ID' and `name.' You can use the `duplicates' command to test whether the \textit{varlist} you use uniquely indentifies observations. The `sort, stable' command can be used, though it is slower.
\item
  Use the `version' command in your .do file to ensure that other researchers who run your code with a newer version of Stata get the same results. 
\item
  Don't use abbreviations for variables (which may become unstable after
  adding variables) or commands (beyond reason)
\item
  Avoid using global macros, and use locals instead. (This is a common piece of programming advice: since globals can be accessed across different functions or spaces, they can create contradictions or inconsistent dependencies. In Stata, it's possible they can be safely used to define directory paths so collaborators can work across different computers, but that should likely be the extent of their use.)
\item
  Use locals for varlists to ensure that long lists of variables include the same variables whenever intended.
\item
  Use computer-stored versions of numerical output instead of manually typing in numbers or copying and pasting. For example, instead of copying and pasting the mean after a `summ' command, refer to `r(mean)'. Use the `return list' command to see a full list of stored values after a regular command and the `ereturn list' after estimation commands.
\item
  If you have a master .do file that calls other .do files, which each have their own .log file, you can run
  multiple log files at the same time (so you have a master .log file)
\item
  Use the `label data' and `notes' commands to label datasets and help yourself and other researchers easily identify the contents.
\item
  Use the `notes' command for variables as well for identifying information that is too long for the variable label.
\item
  Use the `datasignature' command to generate a hash or checksum and help ensure that data is
  the same as before.
\item
  In addition to labeling your variables, you should also use value labels for all categorical variables. Include the
  numerical value in the label, however, since without it, it can be hard to tell what numerical value is actually meant by a given category. 
\item
  Even though Stata is case sensitive, don't use capital letters in variable names since not all software packages
  are case sensitive.
\item
  Make your files as non-proprietary as possible (use the `saveold'
  command to enable those with earlier versions to use your data. This
  is why trusted repositories are so useful--they'll do this for you.)
\end{itemize}

In addition to making code available to the public, the code itself
should be written in a reader-friendly format, referred to as ``Literate
Programming,'' introduced in \cite{knuth_literate_1984} and \cite{knuth_literate_1992}. The basic
idea is that ``the time is ripe for significantly better documentation
of programs, and that we can best achieve this by considering programs
to be \emph{works of literature}\ldots{}Instead of imagining that our
main task is to instruct a \emph{computer} what to do, let us
concentrate rather on explaining to \emph{human beings} what we want a
computer to do.'' {[}emphasis original{]} Simply put, code should be
written in as simple and easily understood a way as possible, and should
be very well commented, so that researchers other than the original
author can more easily understand the goal of the code.

One tool to make literate (statistical) programming significantly easier
is Knitr (see \cite{xie_dynamic_2013, xie_knitr:_2014}) which is built into R
Studio\footnote{R Studio is a popular free integrated implementation of
  R, available at \href{stylesWithEffects.xml}{http://www.rstudio.com}.}.
Knitr uses R Markdown (a very simple plain text markup language,
described at \url{http://rmarkdown.rstudio.com/}) in which one writes
both code and comments that is automatically spun into an easily read
and shareable HTML, PDF, or MS Word document. These can be posted and
shared for free at \href{https://rpubs.com}{RPubs}, an easy to use
hosting service by Rstudio. For Stata users, dynamic documents are slightly less well developed. They were introduced into the official product with the \href{https://www.stata.com/new-in-stata/markdown/}{dyndoc command} in version 15.  \href{http://www.haghish.com}{E.F. Haghish} is actively developing packages (\href{http://www.haghish.com/statistics/stata-blog/reproducible-research/packages.php}{Markdoc, Weaver, Ketchup, and Synlight}) that allow users to write their .do files in such a way that the log files output by Stata are formatted and readable in Markdown, HMTL, or \LaTeX, and other user-written commands as of 2016 were described at a Stata conference (see \href{https://www.stata.com/meeting/oceania16/slides/rising-oceania16.pdf}{slides}).



\subsection{Sharing Data}\label{sharing-data}

In addition to code, researchers should share their data if at all
possible. Many journals do not require sharing of data, but the number
that do is increasing.

\subsubsection{The JMCB Project and
Economics}\label{the-jmcb-project-and-economics}

In the field of economics, few, if any journals required sharing of data
before ``The Journal of Money, Credit, and Banking Project,'' published
in \emph{The American Economic Review} in 1986 \citep{dewald_replication_1986}. \emph{The Journal of Money, Credit, and Banking} started
the \emph{JMCB Data Storage and Evaluation Project} with NSF funding in
1982, which requested data and code from authors who published in the
journal. With a great deal of research funded by the NSF, it should be
noted that they have long had an explicit policy of expecting
researchers to share their primary data\footnote{``Investigators are
  expected to share with other researchers, at no more than incremental
  cost and within a reasonable time, the primary data, samples, physical
  collections and other supporting materials created or gathered in the
  course of work under NSF grants. Grantees are expected to encourage
  and facilitate such sharing.'' See
  http://www.nsf.gov/bfa/dias/policy/dmp.jsp}. Despite this, and despite
the explicit policy of the \emph{Journal} during the project, at most
only 78\% of authors provided data to the authors within six months
after multiple requests. (This is admittedly an improvement over the
34\% from the control group---those who published before the
\emph{Journal} policy went into effect---who provided data.) Of the
papers that were still under review by the \emph{Journal} at the time of
the requests for data, one quarter did not even respond to the request,
despite the request coming from the same journal considering their
paper! The submitted data was often an unlabeled and undocumented mess.
Despite this, the authors attempted to replicate nine papers, and often
were completely unable to reproduce published results, despite detailed
assistance from the original authors.

Shockingly, nothing much changed with the publication of this important
article. A decade later, in a follow-up piece to the JMCB Project
published in the Federal Reserve Bank of St.~Louis \emph{Review}
\citep{anderson_replication_1994}, the authors note that only two economics
journals other than the \emph{Review} itself (\emph{Journal of Applied
Econometrics, Journal of Business and Economic Statistics}) requested
data from authors, and neither requested code. The \emph{JMCB} itself
discontinued the policy of requesting data in 1993, though it resumed
requesting data in 1996. The authors repeated their experiment with
papers presented at the St.~Louis Federal Reserve Bank conference in
1992, and obtained similar response rates as original JMCB Project. The
flagship economics journal, the \emph{American Economic Review} (AER),
did not start requesting data until 2003. Finally, after a 2003 article
showed that nonlinear maximization methods often produce wildly
different estimates across different software packages, that not a
single AER article tested their solution with different software, and
that fully half of queried authors from a chosen issue of the AER,
including a then editor of the journal, failed to comply with the policy
of providing data and code, editor Ben Bernanke made the data and code
policy mandatory in 2005 \citep{mccullough_verifying_2003, mccullough_got_2007}.

The current data policy from the \emph{American Economic Review} can be
seen here: \url{https://www.aeaweb.org/aer/data.php}. The AER conducted a self-review and
found good, but incomplete, compliance \citep{glandon_report_2010}; others believe much work remains. In addition to all
the journals published by the American Economic Association, several top journals, including \emph{Econometrica, The Journal of Applied
Econometrics, The Journal of Money Credit and Banking, the Journal of
Political Economy, The Review of Economics and Statistics, and the
Review of Economic Studies}, now explicitly require data and code to be
submitted at the time of publication. Sadly, one of the top journals, \textit{The Quarterly Journal of Economics} has no data-sharing requirement.

%EXPAND HERE ON JUST HOW MUCH DATA YOU SHOULD SHARE


\subsubsection{General Repositories}\label{general-repositories}

The previous section on the \emph{JMCB} describes only a few journals in
one field of the social sciences. Even if the journal to which you
submit your research does not require you to supply them with your code
and data, researchers should still share these materials. Though some
repositories, particularly Harvard's Dataverse, seem equipped to handle
data from practically any researcher (a free 1 TB of storage is
standard, with more possible upon request), many repositories specialize. \href{http://www.re3data.org}{The Registry of Research Data Repositories}
has described over 900 data repositories to help you find the right data
repository for your data. A key advantage to using a trusted repository
such as one listed here, in lieu of simply throwing the data up on your
own website or making your Dropbox folder public, is that many of these
repositories will take your data in its proprietary (Stata, SAS, SPSS,
etc.) form, and make it accessible in other formats. Storing your data in a repository with other similar datasets also makes it easier for others to find your data, instead of requiring that they already know of its existence, as would likely be the case with personal websites. Your own personal website is also more likely to be taken offline, should a researcher change schools or retire.

\subsubsection{Differential Privacy}\label{differential-privacy}

One important caveat to making data widely available, is that despite
anonymization, in the age of big data, sometimes individual subjects can
easily be identified. \cite{heffetz_privacy_2014} recount deliberate data
releases by Yahoo! Inc., the Massachusetts state government, and
Netflix, that could easily be used to identify individuals in the data,
despite the absence of direct identifiers such as names or social
security numbers. The problem is that ``de-identification does not
guarantee anonymization.'' This problem is well known in computer science, but solutions are still being developed and are not widely implemented.


\subsection{Reporting Standards}\label{reporting-standards}
In research, the devil truly is in the details. Whether it is for assessing the validity of a research design or for
attempting to replicate a study, details of what exactly was done must be recorded and made available to other researchers.
The exact details that are relevant will likely differ from field to field, but an increasing number of fields have
produced centralized checklists that describe (in excruciating detail) what disclosure is required of published studies. These checklists are not often published with the paper, but can be submitted with the original article so that reviewers can check that it has been completed. With infinite and easy web storage, researchers can easily post these materials on their website even if journal editors insist on cutting their methods sections for space reasons. 

\subsubsection{Randomized Trials and CONSORT}\label{randomized-trials}
The most widely adopted reporting standard guideline is the Consolidated Standards of Reporting Trials (\href{http://www.consort-statement.org}{CONSORT}). Parallel to construction of \href{http://clinicaltrials.gov}{clinicaltrials.gov} and registration,
reporting standards evolved, and are now nearly universally adopted for randomized trials published in medical journals, required or requested by reviewers during the review process. This is still in its infancy in the social sciences.

The original CONSORT  was developed in the mid 1990's \citep{begg_c_improving_1996}. After five years, research showed that reporting of essential details, as required by the checklist, had significantly increased in journals requiring the standard  \citep{moher_d_use_2001}. The statement was revised in 2001, and simultaneously published in three of the top journals \citep{moher_consort_2001}). The statement was again revised in 2010 \citep{schulz_consort_2010}. The statement is a 25-item checklist pertaining to the title, abstract, introduction, methods, results, and discussion of the article in question, and seeks to delineate the minimum requirements of disclosure that may not be sufficiently addressed through other measures.

\subsection{Social Science Reporting Standards}\label{soc-sci-standards}
Though a standard akin to CONSORT has not been formally adopted by social science or behavioral science journals, at least as far as I am aware, there have been attempts to do this: In political science, the Experimental Research Section Standards Committee produced a detailed list of items required for disclosure of experiments in political science \citep{gerber_reporting_????}. This checklist is available \href{http://www.davidhendry.net/research-supplemental/gerberetal2014-reportingstandards/gerberetal2014-reportingstandards&appendix1.pdf}{here}.\footnote{\url{http://www.davidhendry.net/research-supplemental/gerberetal2014-reportingstandards/gerberetal2014-reportingstandards&appendix1.pdf}}

In economics, where no standards have been developed, one article has highlighted the fact that there is not much discussion of essential features of randomization (how was randomization stratified, if at all? How were control variables determined?), but no standards have been adopted. \citep{bruhn_pursuit_2009}

In psychological and behavioral research, an extention to CONSORT for Social and Psychological Interventions (CONSORT-SPI) has been developed in \citep{montgomery2013protocol}, but has so far not been widely adopted, or required by journals. 

\subsection{Observational Reporting Standards}\label{observational-standards}
Social science has yet to make a serious push for reporting standards in RCTs, let alone observational work, but the medical/epidemiological literature has created standards in this type of work, though they are not as widely adopted as CONSORT. Perhaps the most well-known is the \href{http://www.strobe-statement.org}{STROBE Statement} (Strengthening the Reporting of Observational Studies in Epidemiology). STROBE provides checklists for reporting of cohort, case-control, and cross-sectional studies. These standards have been endorsed by approximately 100 journals in the field.\footnote{\url{http://www.strobe-statement.org/index.php?id=strobe-endorsement}}

Medicine has in fact come up with too many checklists to describe them all individually. Acknowledging that every field and type of research is different, the Equator Network (Enhancing the Quality of Transparency of Health Research) serves as an umbrella organization that seeks to keep tabs on all the best reporting standards and help researchers find which reporting standard is most relevant for their research. See \url{http://www.equator-network.org/} for more information.





\section{Conclusion}\label{conclusion}

As you may have noticed, many of the activities described in this manual
require extra work. Before you run an experiment, we're telling you to
write down the hypothesis, carefully explain how you are going to test
the hypothesis, write down the very regression analysis you're going to
run, write a detailed protocol of the exact experimental setting, and
then you have to post all of this publicly on the Internet with some
sort of Big Brother organization. Or at least that's one way to look at
it. But I strongly belive these steps are (1) not that difficult once
you get used to them and (2) well worth the reward. You'll get p-values
you can believe in. The next time someone asks you for your data, you
just point them to a website, where they'll download the data and
code, and the code will produce the exact results in the published
paper. The next time you open up a coding file you haven't looked at in
months to make a change suggested by a reviewer, your code will be so
thoroughly commented, you'll know exactly where to go to make the
changes. And the next time you want to extend the analysis of a
published paper, you click the link in the paper and have the data on
your own computer in seconds. Science moves forward.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\appendixpage
\section{Glossary of Terms}
I feel that it is important to define the terms I use in this document. Although many of the concepts overlap, I suggest that researchers use the following terms:
\begin{itemize}
\item
Analysis Plan: See \hyperref[def-pap]{pre-analysis plan}

\item
Data citation: The practice of specifically citing datasets, and not just the paper in which a dataset was used.  This helps other researchers to find data and rewards researchers who share data, leading to better science. Read more at \url{http://www.icpsr.umich.edu/icpsrweb/ICPSR/curation/citations.jsp}. 

\item \label{def-mining}
Data mining: Searching blindly or repeatedly through data to find supposedly statistically significant relationships. While not inherently wrong, if done without a plan or without adjusting for multiple hypothesis testing, test statistics and \hyperref[def-pvalue]{p-values} that result no longer hold their traditional meaning, and can lead to research that cannot be replicated.

\item \label{def-datasharing}
Data sharing: Researchers making the data they use in an analysis available to other researchers, ideally through a trusted public archive. 

\item\label{def-dbp}
Design-based publication: See \hyperref[def-rbr]{results-blind review}.

\item
Disclosure: In addition to the widely accepted norm of publicly declaring all potential conflicts of interest, researchers should detail all the ways in which they test a hypothesis (e.g. include the outcome from all regression specifications tested in the appendix.)

\item
Fishing: See \hyperref[def-mining]{data mining}.

\item
Literate Programming: The idea of writing programming code designed to be read and easily understood by a human. Use of this best practice can make a researcherâ€™s code far more easily reproducible by others.

\item
Multiple hypothesis correction: Statistically taking into account the fact that multiple hypotheses have been tested. This tends to decrease the reported statistical significance of any individual test conducted. The oldest method, known as the Bonferroni correction, simply divides the significance threshold by the number of tests. This is quite conservative, and more modern methods are helpfully described in \cite{anderson_fwer}.

\item
Open Access: Journals, or articles that are freely available to the public, as opposed to available only to paid subscribers. See \href{https://www.plos.org/open-access/howopenisit/}{HowOpenIsIt?} for a detailed definition of the spectrum of openness. 

\item
Open Data: See \hyperref[def-datasharing]{data sharing}.

\item
P-hacking: See \hyperref[def-mining]{data mining}.

\item \label{def-pvalue}
P-value: The statistic researchers use to make judgments regarding statistical significance, which is quite often misunderstood. It is the probability of obtaining a test statistic at least as extreme as the observed test statistic when the null hypothesis is true. 

\item\label{def-pap}
Pre-analysis plan: A document that details, ahead of time, the statistical analysis that will be conducted for a given research project. Outcomes, control variables, and regression specifications are all written in as much detail as possible. This serves to make research more confirmatory in nature. 

\item
Pre-specification: Detailing the method of analysis before actually beginning data work; the same as writing a \hyperref[def-pap]{pre-analysis plan}.

\item
Protocol: A general term meaning a document that provides a detailed description of a research project, ideally written before the project takes place, and in enough detail that other researchers may reproduce the project on their own. Often used in the context of human subjects IRB protocols, but increasingly used in connection with pre-analysis plans. 

\item
Publication Bias: The unfortunate fact that research is often only published when it contains the rejection of a null hypothesis test, i.e. a statistically significant relationship. Reviewers or journal editors may consider a null finding to be of less interest, or a researcher may fail to write up a null result, even though the null result may be the truth.

\item
Registration: Publicly declaring that an investigation of a hypothesis is being or will be undertaken.
\begin{itemize}
\item  Study Registration: Registering a research project generally.
\item  Trial Registration: Registering a randomized trial. 
\end{itemize}
\item
Registry: A database of registered studies or trials. For instance, \url{socialscienceregistry.org} or \url{clinicaltrials.gov}. Some of the largest registries only accept randomized trials, hence the frequent discussion of â€˜trial registries.â€™

\item
Registered Reports: See \hyperref[def-dbp]{results-blind review}.

\item
Replicable: See \hyperref[def-repro]{reproducible}.

\item
Replication: The idea of conducting an existing research project over again, with the hope of obtaining the same result. A subtle taxonomy exists, and unfortunately there is disagreement, which is explained in \cite{hamermesh_viewpoint:_2007} and \cite{clemens_replication}.
\begin{itemize}
\item
Pure Replication: Re-running existing code, with error-checking, on the original dataset and seeing if the published results are obtained.
\item
Scientific Replication: Attempting to reproduce the published results with a new sample, with the same code or with slight variations from the original analysis.
\end{itemize}

\item \label{def-repro}
Reproducible: The test of whether research can redone by another researcher and produce the same results as the original. 
\item
Researcher Degrees of Freedom: the flexibility that a researcher has in data analysis, whether consciously abused or not. See \hyperref[def-mining]{data mining}. 

\item \label{def-rbr}
Results-blind Review: To help reduce publication bias, peer review can take place before results of a study are determined. Reviewers evaluate the design of a study to see whether the question is important and whether the study is well-designed. Good studies are given in-principle acceptance, and cannot be discriminated against for a null result, which, after all, may be the truth. See \url{https://osf.io/8mpji/wiki/home/} for journals practicing this form of publication.

\item Specification searching: Testing numerous regression specifications and reporting only the model that produces the desired results. See \hyperref[def-mining]{data mining}. 
\item
Trusted Digital Repository: A location for storing data that others can believe will not be manipulated, and will be available into the future. Storing data here is superior to simply posting on an individual website, since it is more easily accessible and less easily changed.  
\item
Version Control: A method of tracking every edit made to a computer file. This is often quite useful for empirical researchers who may edit their programming code hundreds or thousands of times.
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\singlespacing
%\section{References}\label{references}
\bibliographystyle{plainnat}
\bibliography{Bibliography}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
