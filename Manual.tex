\documentclass[12pt] {article}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{fullpage}
\usepackage{amsmath}
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}
\usepackage{pslatex}
\renewcommand{\baselinestretch}{2}
\usepackage{setspace}
\usepackage[nottoc]{tocbibind} 
\usepackage{authblk}
\usepackage{rotating}
\usepackage{adjustbox}


\begin{document}
\title{Methods for Transparent and Reproducible Economics Research}

\author{Garret Christensen\footnote{UC Berkeley, Berkeley Initiative for Transparency in the Social Sciences. Please send correspondence to garret@berkeley.edu, or find the version controlled history of this document on \href{https://github.com/garretchristensen/BestPracticesManual}{github}. I gratefully acknowledge helpful comments from Jennifer Sturdy, Alex Wais, and Courtney Soderberg, as well as participants in BITSS seminars and workshops.}}
%\affil[1]{UC Berkeley, Berkeley Initiative for Transparency in the Social Sciences}
\date{\today}
\maketitle
%\begin{center}
%Please send correspondence to garret@berkeley.edu, or find the version controlled history of this document on \href{https://github.com/garretchristensen/BestPracticesManual}{github}.
%\end{center}

\abstract{Economics and the other social sciences have been in the spotlight recently for failures of reproducibility of prominently published research. This article reviews the problems and discusses recent developments and proposed solutions. The problems of publication bias, specification searches (also called data mining or p-hacking), and failures to replicate published research have been known for decades, but a recent surge of research has documented that the problems are still quite prevalent. This has led to a spate of proposed solutions, including study registration, pre-registration of statistical analysis plans, results-blind reviewing, disclosure checklists, and journal requirements of data and code sharing. New methodological tools to test for publication bias, as well as more established tools such as meta-analysis and multiple hypothesis testing adjustment, have become more widely adopted. Lastly, technological improvements such as version control and dynamic documents have made a reproducible research workflow much simpler.}


%\newpage
%\tableofcontents

\newpage
\section{Introduction}\label{introduction}

The principle that scientific claims should be subject to scrutiny by other researchers and
the public at large is well established.  Just examine the Royal Society's motto \textit{nullius in verba} (``take nobody's word for it'') or seminal works on the sociology of science such as  \cite{merton1973sociology}. An important requirement for such scrutiny is that
researchers make their claims transparent in a way that other
researchers are able to use easily available resources to form a
complete understanding of the methods that were used by the original. In
economics, given the personal computing and Internet revolutions and the wide availability of data and processing power, it is essential that data, code, and analyses be transparent \citep{marwick_how}.

This article is intended to be a source for empirical economics researchers who desire to make their own research transparent to, and reproducible by, others. The entire process of research, from
hypothesis generation to publication, is covered. A longer version of this paper, available online, covers issues more thoroughly and address the social sciences more generally in the hopes that quantitative researchers in political science, sociology, and psychology may benefit as well. \footnote{See \url{http://www.github.com/garretchristensen/bestpracticesmanual}.}

While most of us are likely to presume that we ourselves would not
conduct outright fraud, fraud does indeed occur. From making up fake
data to creating bogus e-mail addresses so one could do one's own peer
review, the \href{http://www.retractionwatch.com}{Retraction Watch blog}
documents a distressingly large amount of deliberate fraud in research. This paper will focus on the methodological flexibility and motivated reasoning that is likely far more comon \cite{nosek_scientific_2012}.

The article is laid out as follows: in section~\ref{registration} I discuss one of the major problems in non-transparent research, specifically publication bias. I also discuss how this
problem can be resolved through the practice of registration.
Publication bias stems from the fact that published results are
overwhelmingly statistically significant. But without knowing how many
tests were run (the number of unpublished results), it is impossible to know whether these significant
results are meaningful, or whether they are the 5\% of tests that we
would expect to appear significant due to random sampling, even with no
true effect. By publicly registering all studies, we can have a better
idea of just how many tests have been run.

In section~\ref{rdof} I discuss researcher degrees of freedom and pre-analysis
plans. In addition to registering trials, researchers can also
specify their outcomes of interest and their exact methods of analysis
to bind their hands during the analysis phase by writing a Pre-Analysis
Plan (PAP). This is a relatively new idea in economics, so
there is not yet a consensus on when a PAP should be required, what the
ideal level of detail is, and how much it should constrain a
researcher's hands in the actual analysis, but by pre-specifying analyses,
researchers can distinguish between confirmatory and exploratory
analysis. I do not necessarily place higher intrinsic value on one or
the other, but making the distinction clear is key for appropriate
interpretation.

In section~\ref{replication-and-reproducibility} I discuss workflow and materials sharing, with an eye on
making research replicable by others. Researchers should make their code
and data publicly available so that others may repeat and verify their
analysis. Making data available incentivizes researchers to make their
work accurate in the first place, and makes replication easier for
others, improving the scientific process, but also raises the concern of
differential privacy, since steps should be taken to prevent
identification of individuals in the data. I also discuss the issue of
reporting standards: a standardized list of things that authors should
report to help make their work reproducible.

Section~\ref{conclusion} concludes and presents a vision for moving forward.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Publication Bias and Registration}\label{registration}

\subsection{Publication Bias}\label{publication-bias}

One of the primary drivers of the recent move towards transparency is
increased awareness of publication bias. Numerous papers use collections
of published studies to show that the proportion of significant results
are extremely unlikely to come from any true population distribution
\citep{delong_are_1992, gerber_testing_2001,  ioannidis_why_2005}. By examining the publication rates of null results and
significant results from a large set of NSF-funded studies, \cite{franco_publication_2014} show that the selective publication of only
significant results may stem from the fact that social science
researchers largely fail to write up and submit results from studies
resulting in null findings, citing lack of interest or fear of
rejection. This idea of rejecting, or not even submitting for review, papers with null-results, is commonly referred to as the ``file drawer problem'' \citep{rosenthal1979file}. In fact, the percentage of null findings published in
journals appears to have been decreasing over time, across all
disciplines \citep{fanelli_negative_2012}. It seems unlikely that this would
be an accurate reflection of the state of the universe, unless the hypotheses that scientists are testing are systematically changing over time. 

If journals only
publish statistically significant results, we have no idea how many of
those significant results are evidence of real effects, and which are
the 5\% of random draws that we should expect to show a significant
result with a true population effect of zero. One way to combat this problem is to
require registration of all studies undertaken. Ideally we could then search
the registry for studies of X on Y. If numerous studies show an
effect, we have confidence the effect is real. If 5\% of studies show a
significant effect, we give these outlier studies less credence.

\subsection{Trial Registration}\label{trial-registration}

A basic definition of registration is to publicly declare \emph{all}
research that one plans on conducting. Ideally this is done in a public
registry designed to accept registrations in the given research discipline,
and ideally the registration takes place before data collection begins.

Registration of randomized trials has achieved wide adoption in medicine, but is still
relatively new to economics. After congress passed a law in
1997 requiring the creation of a registry for FDA-regulated trials, and
the NIH created clinicaltrials.gov in 2000, The International Committee
of Medical Journal Editors (ICMJE), a collection of editors of top
medical journals, instituted a policy of publishing only registered
trials in 2005 \citep{DeAngelis2004}, and the policy has spread to
other journals and been generally accepted by researchers \citep{laine_clinical_2007}. Several other countries have their own national trial registries, and the World Health Organization created the \href{http://www.who.int/ictrp/about/en/}{International Clinical Trials Registry Platform (ICTRP)} in 2007 to automatically collect all this information in one place.

An example of the benefit of trial registries is detailed in
\cite{turner_selective_2008}, which details the publication rates of studies
related to FDA-approved antidepressants. (See also \cite{ioannidis_effectiveness_2008}.)
The outcome is unfortunate:
essentially all the trials with positive outcomes were published, about half of questionable-outcome studies were published, and a majority of the
negative-outcome studies were unpublished a minimum of four years after the
study was completed. Figure \ref{fig:turner} shows the drastically different
rates of publication, and a large amount of publication bias.

\begin{figure}
\begin{center}
\includegraphics{TurnerFigure1.PNG}
\end{center}
\caption{Panel A of Figure 1 from \cite{turner_selective_2008}. Figure shows publication rates of studies based on statistical significance of findings.}
\label{fig:turner}
\end{figure}
Of course for this sort of exercise to be possible, unless a reader
merely assumes that a registered trial without an associated published
paper produced a null result (as in \cite{rosenthal1979file}), it requires that the registration site
itself obtain outcomes of trials. \href{http://www.clinicaltrials.gov}{ClinicalTrials.gov} is the only
publicly available trial registry that requires such reporting of
results, and only for certain FDA trials.\footnote{\cite{resultscompliance} finds that compliance with results reporting even among those required was fairly low (22\%). HHS and NIH took steps in November 2014 to expand the amount of results reporting required. See \url{http://www.nih.gov/news/health/nov2014/od-19.htm}}  \cite{hartung_reporting_2014} raises
concerns about discrepancies between reporting of outcomes in published
papers and in the \href{http://www.clinicaltrials.gov}{ClinicalTrials.gov} database; as many as 20\% of
studies had discrepancies in primary outcomes and as many as 33\% had
discrepancies in reporting of adverse events, so there is definitely room for improvement. 

Even with dramatic growth in medical trial registration, problems
remain. \cite{mathieu_s_comparison_2009} looked at trials related
to three medical conditions and found that only 46\% of studies were
registered before the end of the trial with primary outcomes clearly
specified. Even among those adequately registered, 31\% showed some
discrepancies between registered and published outcomes, with bias in
favor of statistically significant definitions.

Almost all registration efforts have thus far been limited to randomized
control trials, as opposed to observational data. Registering all types of analysis could be accepted, though there are definitely concerns about registering observational work---not least of which is the inability to verify that registration preceded analysis. See \cite{dal-re_making_2014} for a recent discussion of the pros and cons.
 
\subsection{Social Science Registries}\label{social-science-registries}

Registries in economics are newer but are growing ever more
popular. A brief overview of the major trial registries is shown in Table~\ref{RegTable}. The Abdul Latif Jameel Poverty Action Lab began hosting a
\href{http://www.povertyactionlab.org/hypothesis-registry}{hypothesis registry}
 in 2009, which was superseded by the American Economic Association's launch of its own \href{http://socialscienceregistry.org}{registry} for randomized trials in May 2013, which had accumulated over 540 studies in 86 countries by
January 2016. The International Initiative for Impact Evaluation (3ie)
launched its own registry for evaluations of development programs, the
\href{http://ridie.3ieimpact.org}{Registry for International Development Impact Evaluations} (RIDIE) in September 2013, which had
approximately 30 evaluations registered in its first year.



\begin{sidewaystable}[]
\centering
\caption{Registries in Medicine and the Social Sciences}
\label{RegTable}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{llllll}
{\bf Registry} & {\bf Sponsor} & {\bf Year Started} & {\bf Study Design} & {\bf Field} & {\bf Studies Registered} \\
ClinicalTrials.gov & National Institutes of Health & 2000 & RCT & Medicine & 206,000+ \\
International Clinical Trials Registry Platform & World Health Organization & 2007 & RCT & Medicine & 284,000+ \\
AEA RCT Registry (SocialScienceRegistry.org) & American Economic Association & 2013 & RCT & Economics & 540+ \\
Registry for International Development Impact Evaluations & 3ie & 2013 & Any & Developing country impact evaluation & 75+ \\
EGAP Design Registry & Experiments in Governance and Politics & 2012 & Any & Political Science & 260+ \\
Open Science Framework & Center for Open Science & 2013 & Any & Any & 3500+
\end{tabular}
\end{adjustbox}
\end{sidewaystable}


In political science, \href{http://e-gap.org/design-registration}{EGAP: Experiments in Governance and Politics} has
created a registry as ``an unsupervised stopgap function to store
designs until the creation of a general registry for social science
research. The EGAP registry focuses on designs for experiments and
observational studies in governance and politics.'' EGAP's registry had over 260
designs registered as of January 2016.\footnote{Earlier less-widely
  adopted attempts to create registries in political science are the
  Political Science Registered Studies Dataverse (PSRSD,
  \href{../customXml/item1.xml}{http://spia.uga.edu/faculty\_pages/monogan/registration.php})
  and the PAP Registry of the Experimental Research section of the
  American Political Science Association
  (\href{numbering.xml}{http://ps-experiments.ucr.edu/browser}).}

Another location for registrations is the \href{http://osf.io}{Open Science Framework} (OSF), created by the \href{http://centerforopenscience.org/}{Center for Open Science}. The OSF serves as a broad research management tool that encourages and facilitates transparency (see \cite{nosek_scientific_2012}.) Registrations are simply unalterable snapshots of research frozen in time, with a persistent URL and timestamp. Researchers can upload their data, code, hypotheses, etc. to the OSF, register it, and then share the resulting URL as proof of registration. OSF registrations can be relatively free-form, but templates exist to conform to standards in different disciplines. Psychology registrations are presently the most numerous on the OSF \footnote{See \url{https://osf.io/explore/activity/\#newPublicRegistrations}.}

\subsection{Meta-Analysis Research}
Another method of detecting and dealing with publication bias is to conduct meta-analysis. This method of research collects all published findings on a given topic, analyzes the results collectively, and can detect, and attempt to adjust for, publication bias in the literature. A handful of organizations specialize in producing these systematic reviews, including the \href{http://www.cochrane.org}{Cochrane Collaboration} for health studies and the \href{http://www.campbellcollaboration.org/}{Campbell Collaboration} for crime \& justice, education, international development, and social welfare research, and the \href{http://www.3ie.org}{International Initiative for Impact Evalution (3ie)} for social and economic interventions in low- and middle- income countries. The US government supports this type of analysis: the Department of Education's Institute of Education Sciences maintains the \href{http://ies.ed.gov/ncee/wwc/}{What Works Clearinghouse}, and the Department of Labor maintains the \href{http://clear.dol.gov/}{Clearinghouse for Labor and Evaluation Research (CLEAR)}, which serve to collect and grade the evidentiary value of research on education and labor, respectively. (The synthesis method in the government clearinghouses is not quite as formally statistical in nature as the previously mentioned Collaborations.)

Although quite common in medical research, the tool is not widely used in some parts of the social sciences. But even in economics, where many graduate students are unfamiliar with the technqiue, important papers exist that have quantitatively synthesized bodies of literature. The unemployment effects of the minimum wage were meta-analyzed in \cite{card1995time}, and the returns to education in \cite{ashenfelter1999review}. A meta-analysis of 87 meta-analyses in economics shows that publication bias is widespread, but not universal. A helpful resource, the Meta-Analysis of Economics Research Network (MAER-Net) which includes meta-analysis datasets and guidelines for economics researchers interested in conducting a meta-analysis is available \href{https://www.hendrix.edu/maer-network/}{here}. Also see \cite{stanley2005beyond}, which helpfully describes the tools of meta-analysis, and is part of a special issue of \textit{The Journal of Economic Surveys} dedicated to meta-analysis. Sol Hsiang, lead author of a prominent meta-analysis of 60 studies measuring the effect of climate on human conflict \citep{hsiang2013climate}, has also developed the \href{http://dmas.berkeley.edu}{Distributed Meta-Analysis System}, an online tool to crowdsource and simplify meta-analysis.

In psychology \cite{simonsohn2014p} also developed a meta-analysis tool called the ``p-curve'' that researchers can use to gauge the evidentiary value of a set of studies by analyzing the distribution of p-values and comparing it to what we would observe under true null effects and likely selectively reported results.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Researcher Degrees of
Freedom}\label{rdof}

Though registration helps solve the problem of publication bias, it does
not solve the problem of fishing for statistical significance within a
given study. This problem with research is known as data mining: repeated searching through statistical or regression
models unknowingly (or deliberately) until significance is obtained.
 \cite{simmons_false-positive_2011} refer to this as exploiting 
``researcher degrees of freedom,'' and it has also been referred to as
``fishing,'' ``p-hacking,'' or ``specification searching'' \citep{humphreys_fishing_2013}. The problem has many names because it can take many shapes. 

Using flexibility around when to stop
collecting data, excluding certain observations, combining and comparing
certain conditions, including certain control variables, and combining
or transforming certain measures, \cite{simmons_false-positive_2011} ``prove'' that listening to the
Beatles' song ``When I'm Sixty-Four'' made listeners a year and a half
younger. The extent and ease of this ``fishing'' is also described in
\cite{humphreys_fishing_2013} who use simulations to show that multiplicity of outcome measures, multiplicity of heterogeneous treatment effects (sub-group analyses), and multiplicity of cut-points for turning a continuous outcome variable into a binary outcome, can all be used to virtually guarantee a false positive, even with large sample sizes. They also find that selective adding of covariates can produce false positives with small samples, though they do find little room to produce false positives through arbitrary selection of model for binary outcomes (linear, logit, or probit) regardless of sample size. \cite{gelman_garden_2013} agree that
``{[}a{]} dataset can be analyzed in so many different ways (with the
choices being not just what statistical test to perform but also
decisions on what data to exclude or exclude {[}sic{]}, what measures to
study, what interactions to consider, etc.), that very little
information is provided by the statement that a study came up with a
\emph{p}\textless{}.05 result.''  What can be
done to solve it? Part of the answer lies in detailed
pre-analysis plans, described below.

\subsection{Pre-Analysis
Plans}\label{pre-analysis-plans}

While registration of studies can help to reduce publication bias or the file drawer problem, a pre-analysis plan (PAP), a detailed outline of the analyses that will be conducted in a study, can be used to reduce researcher degrees of freedom. Registration is now the norm in medicine for randomized trials, and these often include (or link to) prospective statistical analysis plans as part of the project protocol. Official guidance from the US Food and Drug Administration's Center for Drug Evaluation and Research (CDER) from 1998 describes what should be included in a statistical analysis plan and discusses eight items related to data analysis that should be considered: pre-specification of the analysis; analysis sets; missing values and outliers; data transformation; estimation, confidence intervals, and hypothesis testings; adjustment of significance and confidence levels; subgroups, interactions, and covariates; and integrity of data and computer software validity \citep{food1998guidance}.  This is an excellent start, and in the section below I discuss adapting these ideas for a pre-analysis plan in the social sciences.

A PAP contains a specification of the outcomes of the study (sometimes referred to as endpoints in the medical literature), as well as a specification of the methods that will be used to analyze the outcomes. By describing the
method(s) of analysis ahead of time, and to some degree tying the hands
of the researcher, we reduce the ability to data mine. Though one
example of this exists in economics from 2001 \citep{neumark_employment_2001}, the idea
is still quite new to the social sciences. The level of detail varies
widely, and the research community is still constructing norms for
incorporating these documents into final analyses and papers.

\paragraph{What to Include}
Suggestions have been made for the detailed contents of these documents. David McKenzie of the World Bank Research Group proposed a list of ten items that should be included in a PAP, reproduced below from the \href{http://blogs.worldbank.org/impactevaluations/a-pre-analysis-plan-checklist}{World Bank Development Impact Blog}.\footnote{A similar list also appears in \cite{glennerster_running_2013} .}

\begin{enumerate}
\item
  Description of the sample to be used in the study
\item
  Key data sources
\item
  Hypotheses to be tested throughout the causal chain
\item
  Specify how variables will be constructed
\item
  Specify the treatment effect equation to be estimated
\item
  What is the plan for how to deal with multiple outcomes and multiple
  hypothesis testing?
\item
  Procedures to be used for addressing survey attrition
\item
  How will the study deal with outcomes with limited variation?
\item
  If you are going to be testing a model, include the model
\item
  Remember to archive it
\end{enumerate}



\paragraph{Expecting the Unexpected}
\cite{glennerster_running_2013} also mention the ``tension between
the benefits of the credibility that comes from tying ones hands versus
the benefits of flexibility to respond to unforeseen events and
results.'' Writing a PAP can lend extra credibility to research by making it confirmatory in nature as opposed to exploratory.
Both types of research are absolutely valuable, but knowing the distinction is important. If some sort of restriction on the data is specified ahead of time based on theory or previous research, be it a specific functional form, exclusion of outliers, or an interaction term (subgroup analysis) that turns a null effect for the population into a significant effect for some subgroup, this can be considered confirmatory research. 

Some would say this is of more value than the exploratory research approach of simply running 20 sub-group analyses and finding that one or two are significant. This may be an estimate of a true effect, but should be labeled as exploratory, and future researchers could attempt to confirm this finding by addressing the question of the sub-group specifically. The potential downside to pre-stating hypotheses and analysis plans is that no matter how carefully researchers plan ahead, something truly unexpected can occur. (An example discussed at a recent conference was subjects showing up for an experiment intoxicated. One example from a field experiment involved fatalities from a lightning strike at a school that was part of a competitive girls scholarship program \citep{kremer2009incentives}.) This is why, even though I may use the phrase ``bind our hands,'' I suggest that researchers not be punished for conducting research outside the analysis plan. I instead recommend that researchers clearly delineate which analysis was included in the analysis plan, and which was not, so that readers can know what is confirmatory and what is exploratory.


\paragraph{When to Write}
There is some question as to when one should write one's pre-analysis plan. ``Before you begin to analyze your data'' seems like the obvious answer, but this should be precisely defined. One could write the PAP before any baseline survey takes place, after any intervention but before endline, or after endline but before analysis has begun. \cite{glennerster_running_2013} and \cite{OlkenPAP} have an informative discussion of the relative values of PAP timing. If one writes the PAP before the baseline, this is in some sense the purest, most free from accusations of p-hacking, but one could also miss valuable information. For example, suppose in baseline one learns that the intended outcome question is phrased poorly and elicits high rates of non-response, or that there is very little variation in the answers to a survey question. If the PAP was written after baseline, one could have accounted for this, but at the same time, researchers would also be free to change the scope of their analysis--for example, in the baseline survey of a field experiment designed to increase wages revealed that few of the subjects worked outside the home, the researcher could change the focus of the analysis. This is not necessarily wrong, but it changes the nature of the analysis somewhat.

PAPs could also be written after endline data has been collected but before the investigators have begun to analyze the data. Some have suggested that one could even look at baseline data from the control group only before writing the PAP. This may be problematic, however, since a researcher could learn that the control group had a particularly low or high value of a certain outcome variable, and then choose to include or not include this variable in the analysis as a result. The original research design could have been intended to analyze the increase in secondary school attendance, but looking at the control group, the researcher sees that the control group had a very high rate of attendance, making a significant difference between control and treatment (the treatment effect) unlikely. Learning this after the experiment has concluded and searching for things that might be easily different between treatment and control is more exploratory than confirmatory. 

An alternative proposal discussed in \cite{OlkenPAP} is to remove the treatment status variable from the dataset before looking at the data, which seems to alleviate some of the concerns. However, one could still search for sub-group analyses at this stage. If you parse the outcome data by gender, and males and females have a similar distribution, to find a differential treatment effect by gender would seem unlikely. If male and female had wildly different outcomes, it would seem like a significant interaction is more likely. This is more exploratory than confirmatory. 


\subsubsection{Examples}\label{examples}
Examples of pre-analysis plans in economics are relatively rare, but several examples of published papers resulting from studies with PAP exist. The items below come from the \href{http://www.povertyactionlab.org/Hypothesis-Registry}{J-PAL Hypothesis Registry}; I highlight those that have publicly available final papers and make reference to the PAP in the paper. 


\begin{itemize}
\item 
 \cite{casey_reshaping_2012} includes evidence from a large-scale field experiment on community driven development projects in Sierra Leone. The analysis finds no significant benefits. Given the somewhat vague nature of the development projects that resulted from the funding, and the wide variety of potential outcomes, finding significant results would have been relatively easy. In fact, the paper includes an example of how, if they had the latitude to define outcomes without a pre-analysis plan, the authors could have reported either large and significantly positive or negative outcomes, depending on their preferences. The paper also includes a discussion of the history and purpose of pre-analysis plans. The \href{http://emiguel.econ.berkeley.edu/assets/miguel_research/8/_Appendix__Reshaping_Institutions_-_Evidence__on__Aid__Impacts__Using__a__Pre___Analysis__Plan.pdf}{online appendix}
contains the PAP.

\item 
Oregon expanded its Medicare enrollment through a random lottery in 2008, providing researchers with an ideal avenue to evaluate the benefits of enrollment. \cite{finkelstein_oregon_2012, doi:10.1056/NEJMsa1212321} and \cite{Taubman17012014} show that recipients did not improve in physical health measurements, but were more likely to have insurance, had better self-reported health outcomes, utilized emergency rooms more, and had better detection and management of diabetes. Pre-analsyis plans from the project are available at the \href{http://www.nber.org/oregon/documents.html}{National Bureau of Economics}' site devoted to the project. (See, for example, \cite{taubman_oregon_2013, baicker_katherine_oregon_2014}.)


\item
The shoe company Toms funded a rigorous evaluation of its in-kind shoe donation program. Researchers wrote a pre-analysis plan before conducting their research, and found no evidence that shoe donations displace local purchasing of shoes. See \cite{wydick_-kind_2014, katz_elizabeth_pre-analysis_2013}. The PAP is available in the \href{http://www.povertyactionlab.org/doc/pre-analysis-planwydick2-12-13pdf}{JPAL Hypothesis Registry}. This is one of many projects that has benefited from a pre-analysis plan because of the involvement of a group with a vested interest, such as a government or corporation. Even researchers skeptical of the need for PAPs in general admit the benefit to publicly pre-stating analysis plans when someone involved has such a clear incentive for results to go a certain direction. 

\item
Researchers from UC San Diego and the World Bank evaluated job training programs run by the Turkish government and found only insignificant improvements and a strongly negative return on investment. See \cite{almeida_impact_2012, vocationalTurkey}. The PAP is available in the \href{http://www.povertyactionlab.org/Hypothesis-Registry}{J-PAL Hypothesis Registry} as well as the \href{http://blogs.worldbank.org/impactevaluations/files/impactevaluations/iskurie_analysisplan_v4a.pdf}{World Bank Development Impact Blog}.



%\item 
%Teams led by Ben Olken have evaluated multiple randomized interventions in Indonesia. The \href{http://www.povertyactionlab.org/evaluation/project-generasi-conditional-community-block-grants-indonesia}{Generasi program} linked community block grants to performance. The PAP are \cite{olken_generasi_2009, olken_generasi_2010} and are available in the \href{http://www.povertyactionlab.org/Hypothesis-Registry}{J-PAL Hypothesis Registry}. The researchers found health improvement, but no education improvement \citep{olken_indonesias_2010, olken_should_2014}.

%\item 
%Another project in Indonesia used a field experiment to evaluate different means of poverty targeting for cash transfer programs: proxy-means testing, community-based targeting, and a hybrid of the two. Results show that the proxy-means testing outperformed the other methods by 10\%, but that community members were far more satisfied with the community method. The PAP and final paper are available as \cite{olken_targeting_2009} and \cite{alatas_targeting_2012}

%\item
%An example from pyschology is a pre-registered replication of an implicit association test. Existing research showed evidence of stronger racial preferences among fertile women. \cite{NosekPreRegistered} failed to reproduce this effect in four tries, suggesting the association is weaker than originally found. The resulting manuscript, as well as the time-stamped registration of the analysis plan, can be found on the Open Science Framework at \url{https://osf.io/g3sca/}.
\end{itemize}

Additionally, Alejandro Ganimian developed \href{http://scholar.harvard.edu/files/alejandro_ganimian/files/pre-analysis_plan_template_0.pdf}{a template for pre-analysis plans} that instructors may find useful when teaching tranpsarency methods, or researcher themselves may find useful when developing their own pre-analysis plan.

\subsubsection{Observational Pre-Analysis Plans}
As with registration, part of the concern with pre-analysis plans is whether the ``pre'' aspect is verifiable. How can we be sure that a researcher didn't look at observational data, run a few hypothesis test, and then write the PAP? One way is to use reliably known availability dates of certain datasets such as government administrative data. Verifiable pre-specification of observational research was undertaken in \cite{neumark_employment_2001}, which prospectively detailed analysis of the unemployment effects of the minimum wage, a question that has been debated for at least a century \citep{Neumark_2014_bathwater}.\footnote{An editorial introduction to \cite{neumark_employment_2001} was accidentally left out of the issue, and published the following month, see \cite{Levine2001editorial}. Others followed Neumark's paper in spirit by using the same regression specifications on data from Canada \citep{CanadianMinWage2006}.} 

The federal minimum wage changed in October 1996 and September 1997. Neumark submitted a pre-specified research design consisting of the exact estimating equations, variable definitions, and subgroups that would be used to analyze the effect of the minimum wage on unemployment of younger workers using October, November, and December CPS data from 1995 through 1998. This detailed plan was submitted to journal editors and reviewers prior to the end of May 1997; the October 1996 data started to become available at the end of May 1997, and Neumark assures readers he had not looked at any published data at the state level prior to submitting his analysis plan. 

Neumark's hope was to eliminate the potential for bias due to ``author effects.'' The obvious time stamp of the federal governmentâ€™s release of data indeed makes this possible, but the situation also benefits from the depth and intensity of the debate prior to this study. Neumark had an extensive extant literature to rely on when picking specific functional forms and subgroup analyses to be conducted. He tested two definitions of the minimum wage, the ratio of the minimum wage to the average wage (common in Neumark's previous work), as well as the fraction of workers who benefit from the newly raised minimum wage (used in David Card's earlier work \citep{Card1992minwage}) and tests both models with and without controls for the employment rate of higher-skilled prime age adults (as recommended by \cite{Deere1995minwage}). The results show mostly insignificant results, but that 18 of the 80 specifications result in statistically significant decreases in employment (at the .10 or .05 level), with elasticities ranging from -0.14 to -0.3 for significant estimates and smaller (closer to zero) for insignificant estimates. 

How widely adoptable is Neumark's method of pre-specification, and what do we gain from it? Thanks to pre-specification we know exactly how many hypothesis tests Neumark ran (at least 80 coefficients, and more with joint tests), so we know the context in which to appropriately interpret the p-values. This is a significant improvement over much other observational research. Of course, econometricians could still argue about the appropriateness of certain models (whether to include lags or look at contemporaneous effects, for instance) and the analysis suffers from noisy estimates given the relatively small number of changes to state minimum wages compared to other time periods that had been previously analyzed using similar methods.  

It is also difficult to see how a researcher could reach this level of detail with a research question with which they were not already intimately familiar; it seems more likely that one would either pre-specify with either a completely inadequate level of detail, or a wildly inappropriate specification. But perhaps this familiarity is a good thing. Much exploratory observational research could continue as is, and for important and thoroughly debated and defined question, confirmatory research could be conducted in a pre-specified fashion. Although pre-specification is difficult, given the vast troves of data released on reliable schedules by governments of developed countries through the world, verifiable pre-specification of observational research could indeed be conducted.

\subsubsection{Project Protocols}\label{project-protocols}

A project protocol can be somewhat similar to a PAP, but is distinct. A protocol is a detailed recipe or instruction manual for others to use to reproduce an experiment. Protocols are important both in helping solve researcher degrees of freedom problems by making the exact details of analysis known and help avoid selective reporting, as well as in making one's work reproducible. Protocols are standard in the medical literature, as in areas of lab science, but may be less familiar to those used to working with administrative or observational data. Lab sciences are rife with examples of experiments failing to replicate because of supposedly minor changes such as the brand of bedding in mouse cages, the gender of the laboratory assistant, or the speed at which one stirs a reagent \citep{sorge2014olfactory, hines2014sorting}, and the same situation may exist in the social sciences. \textit{Nature} has decided to expand its methods section in order to encourage better reporting.\footnote{\url{http://www.nature.com/news/announcement-reducing-our-irreproducibility-1.12852}}

The social sciences may benefit from more careful documentation of methods. When one uses administrative data this can be accomplished by sharing one's data and code so that analysis is transparent.\footnote{It should be noted that the need for documentation of survey method is not eliminated by using administrative data, the burden simply falls upon the administration.} This is discussed below in Section~\ref{replication-and-reproducibility}. With original data collection, researchers should provide very detailed descriptions of what exactly they did. A 33-item checklist of suggested items is contained in the SPIRIT (\href{http://www.spirit-statement.org}{Standard Protocol Items: Recommendations for Interventional
Trials}) statement \citep{chan_spirit_2013}, including details on the participants, interventions, outcomes, assignment, blinding, data collection, data management, and statistical methods, among other things. 

A specific area in which social sciences could improve involves details of randomization. \cite{bruhn_pursuit_2009} documents the lack of clear explanation pertaining to how randomization was conducted in randomized controlled trials (RCTs) published in economics journals. Variables used for stratification are not described, and the decision of whether to control for baseline characteristics was often done after the fact. While there seems to be internal disagreement in both
medicine and the social sciences over the appropriateness of including baseline
control variables in regression analysis of a randomized trial, having
researchers selectively report whatever method gives them the most
significant-seeming results is obviously not the optimal outcome. 

The medical literature also exhibits much greater concern over the blinding and concealment of randomized assignment than some of the social science literature. In some situations, blinding is impossible or irrelevant in a social science field experiment: for example, the recipient of a cash transfer needs to know that they received cash in order for the program to have any effect. Also, a social scientist interested in the potential scaling up of a government program may rightfully be unperturbed by some respondents assigned to the control group somehow gaining access to treatment, since this behavior would undoubtedly occur if the program were scaled up and the researcher still has a valid intention to treat estimate.  This is not always the case, especially if one wants an accurate estimate of the efficacy of a program or a treatment on the treated estimate. Tales of trials ruined through carelessness with the original randomization assignment as well as tips on how to avoid the same problem are described in \cite{schulz_allocation_2002}.\footnote{In addition to \cite{bruhn_pursuit_2009}, political science has produced guidelines for randomization and related disclosure, avaialable at \url{http://e-gap.org/resources/guides/randomization/}.}


 Some medicine and science journals have begun to publish protocols.   While the
advantages of publishing a protocol related to the development of a new
procedure (e.g. ``we have developed a new method of isolating mRNA'')
is clear, the advantages of publishing protocols for randomized
trials under way are perhaps less obvious, but still exist. \emph{BioMed
Central} and \emph{BMJ Open}, among others, now publish protocols of
trials planned or ongoing, with the hopes that this will reduce
publication bias, allow patients to see trials in which they might like
to enroll, allow funders and researchers to learn of work underway to
avoid duplication, and to allow readers to compare what research was
originally proposed to what was actually completed.\footnote{See
\url{http://www.biomedcentral.com/authors/protocols} and
\url{http://bmjopen.bmj.com/site/about/guidelines.xhtml\#studyprotocols}.}
\emph{BMJ Open} suggests, but does not require, that its published
protocols include the items in the SPIRIT checklist.

Protocols are not a perfect solution, as even in published (or otherwise public) protocols, studies have found important differences between protocols and published results. 60-71\%
of outcomes described in protocols went unreported in the paper while
62\% had major discrepancies between primary outcomes in the protocols
and in the published papers, though there was a relatively even mix of
these discrepancies favoring significant or insignificant results \citep{chan_a_empirical_2004}. Another study found that appropriate level of statistical detail is often lacking in protocols, and there are often discrepancies between protocols and published results \citep{saquib_practices_2013}. 31\% of published papers had some sort of pre-specified plan for their regression adjustments (i.e.~specifying which baseline covariates would be controlled for), %while 70-74\% of those that published a design paper or provided a protocol to the authors, 
but only 53\% of the plans matched what was published in the ultimate paper.


\subsection{Multiple Hypothesis Testing}
Several of the PAP and lists of suggestions above include corrections for multiple hypothesis testing. The idea of correcting for multiple tests is widespread in certain fields, but has yet to take hold in the social sciences. Simply put, the idea is that because we are aware of the fact that test statistics and p-values appear significant purely by chance a certain proportion of the time, we can report different, \textit{better} p-values that control for the fact that we are running multiple tests. There are several ways to do this, a few of which are used and explained in a simple and straightforward manner by \cite{anderson_fwer}:
\begin{itemize}
\item
Report index tests---instead of reporting the outcomes of numerous tests, standardize outcomes and combine them into a smaller number of indexes (e.g. instead of separately reporting whether a long-term health intervention reduced blood pressure, diabetes, obesity, cancer, heart disease, and Alzheimer's, report the results of a single health index.) \cite{kling2007experimental} implements an index test from the Moving to Opportunity field experiment, using methods developed in biomedicine by \cite{obrien1984procedures}.

\item
Control the Family-Wise Error Rate (FWER)---FWER is the probability that at least one true hypothesis in a group is rejected (a type I error), meaning it is advisable when the damage from incorrectly claiming \textit{any} hypotheses are false is important. There are several ways to do this, with the simplest (but very conservative) method being the Bonferroni correction of simply multiplying every original p-value by the number of tests done. Holm's sequential method involves ordering p-values by class and multiplying the lower p-values by higher discount factors \citep{holm_multipletesting}. An efficient recent method is the free step-down resampling method, developed by \cite{westfall_young_multiple}.
  
\item
Control the False Discovery Rate (FDR)---In situations where a single type I error is not catastrophic, researchers may be willing to use a less conservative method and trade off some incorrect rejections in exchange for greater power. This is possible by controlling the FDR, or the percentage of rejections that are type I errors. \cite{benjamini1995controlling} details a simple algorithm to control this rate at a chosen level, and \cite{benjamini2006adaptive} describes a two-step procedure with greater power. 


\end{itemize}

\subsection{Subgroup Analysis}
One aspect of researcher degrees of freedom related to multiple hypothesis testing that seems to have taken hold widely in the medical literature is the aversion to sub-group analysis (``interactions'' to
most economists). Given the ability to test for a differential effect by many different groupings, crossed with each outcome variable, sub-groups analysis can almost always find some sort of supposedly significant effect. An oft-repeated story in the medical literature revolves around the publication
of a study on aspirin after heart attacks. When the editors suggested including 40 subgroup analyses, the authors relented on the condition
they include some of their own. Gemini and Libras had worse outcomes
when taking aspirin after heart attacks, despite the large beneficial
effects for everyone else. (Described in \cite{schulz_multiplicity_2005},
with the original finding in \cite{isis-2_second_international_study_of_infarct_survival_collaborative_group_randomised_1988}) Whether in a randomized trial or
not, social scientists could benefit from reporting the number of
interactions tested, possibly adjusting for multiple hypotheses, and
ideally specifying beforehand the interactions to be tested, with a justification from theory or previous evidence as to why the test is of interest. 


\subsection{Results-Blind Reviewing}
A new development in research transparency which helps to address both publication bias and researcher degrees of freedom is results-blind reviewing. In results-blind reviewing, authors submit a detailed research plan \textit{before} conducting the research. They submit this plan to a journal, and the journal rejects or gives an in-principle acceptance of the not-yet-written article based on the scientific merit of the question being asked and the methods proposed to answer them, as opposed to whether the results pass an arbitrary threshold of statistical significance. Then the authors conduct the research, and their paper is published as long as they don't deviate too much from what they initially proposed. The editors and reviewers have tied their hands and have no ability to accept only significant results, and the authors have less incentive to game their statistical analysis in order to find something significant. 

This new mode of publication, called ``Registered Reports," is championed by Chris Chambers, psychologist at Cardiff University, and has been adopted by over a dozen journals.\footnote{See a full list of journals adopting the procedure at \url{https://osf.io/8mpji/}.} \textit{Social Pyschology} ran an issue dedicated to this type of article, with an editorial explaining the concept \citep{nosek2014registered}. \textit{AIMS Neuroscience} published an editorial answering 25 frequently asked questions pertaining to registered reports \citep{chambers2014instead}. A forthcoming special issue of \textit{Comparative Political Studies} will also publish articles of this type \citep{FindleyCPS}. 

  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Replication and
Reproducibility}\label{replication-and-reproducibility}

\begin{quote}
``Economists treat replication the way teenagers treat chastity - as an
ideal to be professed but not to be practised.''---Daniel Hamermesh,
University of Texas at Austin Economics
\end{quote}

\begin{quote}
``Reproducibility is just collaboration with people you don't know,
including yourself next week''---Philip Stark, University of California Berkeley Statistics
\end{quote}

Replication, in both practice and principle, is a key to social
science research. I first define what exactly we mean by replication
using the taxonomy developed in \cite{hamermesh_viewpoint:_2007} and \cite{hunter_desperate_2001}.
According to Hamermesh, replication comes in a few different shapes: pure, statistical, and
scientific.

\begin{itemize}
\item
  Pure: Using the exact same data and the exact same model to see if the
  published results are reproduced exactly.
\item
  Scientific: Using a different sample from a different population, and
  similar, but perhaps not identical model .
\item
  Statistical: Using the same model and underlying population but a
  different sample. In Hamermesh's view, this is less relevant to certain fields, such as economics,
  where researchers are likely to already use as large a sample as is available.
\end{itemize}

One might imagine a fourth type that uses the same data but probes the data using additional robustness and sensitivity checks. Others have described replication in terms of a spectrum from full replication (independent collection of data and re-running analysis) to
reproducibility, where the same data or code are re-used by other
researchers \citep{peng_reproducible_2011}, and another taxonomy is proposed in \cite{ClemensRepTax}. Whatever the terminology used, replication or reproducibility, transparent research requires making data and code available to other researchers so they can try and get the same results.


\subsection{Code and Workflow}\label{code-and-workflow}

Reproducing research often involves using the exact code and statistical
programming done by the original researcher. To make this possible, code
needs to be both (1) easily available and (2) easily interpretable.
Thanks to several free and easy to use websites described below, code
can easily be made available by researchers without requiring funding or
website hosting. Making code easily interpretable is a somewhat more complicated
task, nevertheless, the extra effort spent to make a more manageable
code pays off with large dividends.

\subsubsection{Publicly Sharing Code}\label{publicly-sharing-code}

Once analysis is complete (or even before this stage) researchers should
share their data and code with the public. GitHub
(\url{http://www.github.com}), The Center for Open Science's Open
Science Framework (\url{http://osf.io}), and Harvard University's
Dataverse (\url{http://thedata.org}) are all free repositories for data
and code that include easy to use version control.\footnote{BitBucket
  (\href{styles.xml}{http://www.bitbucket.org}) is another
  web service that one can use for free version control and archiving
  of public data and code.} 
  
Version control is a powerful way to archive versions of files so that old versions are not lost and can be returned
to if needed. The most naive way to organize a project would be to just name a file ``MyAnalysis.do'' and then save over it any time you or your coauthor made changes. This of course would result in a huge loss of information. Most people are probably doing some version of the ``date and initials'' method. Instead of simply calling one's analysis code ``MyAnalysis.do'' and repeatedly saving over and losing old versions, when you save a new version, you add the date to the file name: ``MyAnalysis.2014.08.13.do'', and if a co-author makes changes on the same day, they add their initials:``MyAnalysis.2014.08.13\_GC.do'' or they change the date: ``MyAnalysis.2014.08.14.do''. This can work on small projects, but it is not the best way to work. Why does the operating system say the file from August 13 was modified in October? What if one analysis script file is called 14 times in 9 different files---did you remember to go through and update all the calls? \cite{GentzkowShapiro} provide strong evidence of the inadequacy of dating and initialing : ``\textit{Not one piece of commercial software you have on your PC, your phone, your tablet, your car, or any other modern computing device was written with the ``date and initial'' method.}'' [emphasis original]

The solution is version control (also referred to as revision control). Version control is free software (\href{https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control}{Git} and \href{https://mercurial.selenic.com/}{Mercurial} are popular and free implementations) that stores all versions of files you create, and can easily compare versions of the files to highlight changes, revert to earlier versions, accept or reject changes proposed by a collaborator, and reconcile conflicting versions when different people make changes to the same file. Version control creates a repository, and stores all versions of your files in the repository\footnote{Repositories can take up a significant amount of space, but you can avoid this problem by not storing generated files or binaries (.pdf, .docx, .etc) since every user with access to the repository should be able to recompile and generate files on their own. Version control works best with simple text files.}  Web services
such as \href{http:www.github.com}{GitHub} offer free hosting of these repositories with easy to use web-based interfaces and GUI software, so that you and your collaborators can all access the same files, but you can easily host the repository on a server of your own. 

\subsubsection{Managing Workflow}\label{managing-workflow}

Code is just one aspect of a larger structure referred to as
``workflow'' in \cite{long_workflow_2008}, by which is meant the
combination of data, code, organization, and documentation: everything
from file and variable names to folder organization as well as efficient
and readable programming, and data storage and documentation. Stata-users should find \cite{long_workflow_2008} useful, while R users can refer to \cite{gandrud2013reproducible} for workflow recommendations both general and specific to their respective programming language.

Another valuable, more brief work is Matthew Gentzkow and Jesse Shapiro's manual on code and data \href{http://faculty.chicagobooth.edu/jesse.shapiro/research/CodeAndData.pdf}{\citep{GentzkowShapiro}}. They come from the same background as many economists--they did not take many programming classes, they are interested in tools only to accomplish their applied research goals. Once their data became massive, and their programming problems became massive with it, they figured they should listen to the fulltime programmers and database managers who had spent years and billions of dollars solving these problems. Their manual adapts many of these solutions to data-based social science research.   

I also refer undergraduate instructors and others who may be interested to Richard Ball and Norm Medeiros' \href{http://www.haverford.edu/TIER/}{Project TIER} (Teaching Integrity in Empirical Research), which is a ``protocol for comprehensively documenting all the steps of data management and analysis that go into an empirical research paper.'' A specific file organization using the Open Science Framework is taught so that teachers can exactly reproduce the work of every student (and so students can reliably get the same answer every time they conduct their analysis). 

\paragraph{Software:}
The movement by many towards open source software such
as \href{http://www.r-project.org/}{R} and \href{https://www.python.org/}{Python} may lead to reproducibility and access gains over time. However, many disciplines have long
traditions of using proprietary software such as SAS and STATA, and
learning a new programming language may be an undesirable additional
task in researchers' busy lives. That said, there are several general
coding rules that all researchers should use when organizing and
implementing their analysis, and researchers should strive to make their
work usable by as many others as possible.

\paragraph{Writing Code:}
Perhaps the most important rule is to write code instead of working by hand. By
that I mean:

\begin{itemize}
\item
  Do not modify data by hand, such as with a spreadsheet. Which is to
  say, don't use Excel.
\item
  Use neither the command line nor drop-down menus nor point-and-click
  options in statistical software.
\item
  Instead, do everything with scripts.
\end{itemize}

The simple reason for this is reproducibility. Modifying data in Excel
or any similar spreadsheet program leaves no record of the changes made
to the data, nor any explanation of the reasoning or timing behind any
changes. Although it may seem easy or quick to do a one-time-only
cleaning of data in Excel, or make ``minor'' changes to get the data
into a format readable by a researcher's preferred statistical software,
unless these changes are written down in excruciating detail, this is
not reproducible by other researchers. It is better to write a programming
script that imports the raw data, does all necessary changes, with
comments in the code that explain changes, and saves any intermediate
data sets used in analysis. Then, researchers can share their initial
raw data and their code, and other researchers can reproduce their work
exactly.

Though a fair amount of research has been done using
pull down menus in SPSS or Stata, it generally makes research less reproducible. A bare minimum
if one insists on going this route is to use the built-in
command-logging features of the software. In Stata this involves the
`cmdlog' command; in SPSS this involves the paste button to add to a
syntax.

The ideal is to make everything, including changes like rounding and
formatting, done with scripts. Even downloading of data from websites
can be done through a script. For example, in R, the download.file()
function can be used to save data from a website. (Though of course this
opens the possibility to the data file changing. When reproducing
results from a given dataset is more important than the data from a
specific source, researchers should download their raw dataset once, and
never save over it, instead saving all modified intermediate datasets in
a separate location.) 

Another important way to prevent
unintentional changes to data is to always set the seed for random
number generators whenever any random numbers are to be used (set.seed()
in R, set seed () in Stata). Additionally, information about the exact
software version used should be included (include the `version' command in Stata, or use the
session.info() command in R) as well as computer processor and operating
system information. The casual programmer may assume that sophisticated
software would always produce the exact same answer across multiple
versions of software and platforms, but this is not the case. This is also definitely not the case with user-written packages. R users can use the packageVersion() command, and can run old versions of packages since they are archived at \href{http://cran.r-project.org}{CRAN}. Stata users can use the viewsource command for any .ado they use, but since the Statistical Software Components (SSC) unfortunately does not archive old versions, reproducibility may be lost, so ideally researchers would include the actual code for the version of the user-written .ado along with their publicly archived data and code files


Finally, two simple organizing principles to consider are:
\begin{enumerate}
\item 
 Consider not saving statistical output, and just saving the code and data that generates it. Obviously this would be unrealistically time consuming for large projects, but the idea is that you should be able to reproduce all steps of your analysis such that you could in theory take this approach.

\item
What would happen if you, or your laptop hard drive, were hit by a bus? How easily would anyone else be able to reproduce your work? Hopefully the probability is non-zero.
\end{enumerate}

\subsubsection{Dynamic Documents}\label{general-workflow-suggestions}

In addition to making code available to the public, the code itself
should be written in a reader-friendly format, referred to as ``Literate
Programming,'' introduced in \cite{knuth_literate_1984} and \cite{knuth_literate_1992}. The basic
idea is that ``the time is ripe for significantly better documentation
of programs, and that we can best achieve this by considering programs
to be \emph{works of literature}\ldots{}Instead of imagining that our
main task is to instruct a \emph{computer} what to do, let us
concentrate rather on explaining to \emph{human beings} what we want a
computer to do.'' {[}emphasis original{]} Simply put, code should be
written in as simple and easily understood a way as possible, and should
be very well commented, so that researchers other than the original
author can more easily understand the goal of the code.

One way to make literate (statistical) programming significantly easier
is with dynamic documents, which combine code and output into one automated document. A prominent system is Knitr (see \cite{xie_dynamic_2013, xie_knitr:_2014}), which is built into R
Studio\footnote{R Studio is a popular free graphical integrated implementation of
  R, available at \href{stylesWithEffects.xml}{http://www.rstudio.com}.}.
Knitr uses R Markdown\footnote{R Markdown is a very simple plain text markup language,
described at \url{http://rmarkdown.rstudio.com/})} in which one writes
both code and comments that is automatically spun into an easily read
and shareable HTML, PDF, or MS Word document. These can be posted and
shared for free at \href{https://rpubs.com}{RPubs}, an easy to use
hosting service by Rstudio. For Stata users, dynamic documents are slightly less well developed, but \href{http://www.haghish.com}{E.F. Haghish} is actively developing packages (\href{http://www.haghish.com/statistics/stata-blog/reproducible-research/packages.php}{Markdoc and Weaver}) that allow users to write their .do files in such a way that the log files output by Stata are formatted and readable in Markdown, HMTL, or \LaTeX . 


\subsection{Sharing Data}\label{sharing-data}

In addition to code, researchers should share their data if at all
possible. Many journals do not require sharing of data, but the number
that do is increasing. Most recently, a consortium of top medical journal editors has proposed a new data-sharing requirement that, if adopted, could drastically improve data availability in medical research \citep{ICMJEData}.

\subsubsection{The JMCB Project and
Economics}\label{the-jmcb-project-and-economics}

In the field of economics, few, if any journals required sharing of data
before ``The Journal of Money, Credit, and Banking Project,'' published
in \emph{The American Economic Review} in 1986 \citep{dewald_replication_1986}. \emph{The Journal of Money, Credit, and Banking} started
the \emph{JMCB Data Storage and Evaluation Project} with NSF funding in
1982, which requested data and code from authors who published in the
journal. With a great deal of research funded by the NSF, it should be
noted that the NSF has long had an explicit policy of expecting
researchers to share their primary data\footnote{``Investigators are
  expected to share with other researchers, at no more than incremental
  cost and within a reasonable time, the primary data, samples, physical
  collections and other supporting materials created or gathered in the
  course of work under NSF grants. Grantees are expected to encourage
  and facilitate such sharing.'' See
  http://www.nsf.gov/bfa/dias/policy/dmp.jsp}. Despite this, and despite
the explicit policy of the \emph{Journal} during the project, at most
only 78\% of authors provided data to the authors within six months
after multiple requests. (This is admittedly an improvement over the
34\% from the control group---those who published before the
\emph{Journal} policy went into effect---who provided data.) Of the
papers that were still under review by the \emph{Journal} at the time of
the requests for data, one quarter did not even respond to the request,
despite the request coming from the same journal considering their
paper. The submitted data was often an unlabeled and undocumented mess.
Despite this, the authors attempted to replicate nine papers, and often
were completely unable to reproduce published results, despite detailed
assistance from the original authors.

Unfortunately, nothing much changed with the publication of this important
article. A decade later, in a follow-up piece to the JMCB Project
published in the Federal Reserve Bank of St.~Louis \emph{Review}
\citep{anderson_replication_1994}, the authors note that only two economics
journals other than the \emph{Review} itself (\emph{Journal of Applied
Econometrics, Journal of Business and Economic Statistics}) requested
data from authors, and neither requested code. The JMCB itself
discontinued the policy of requesting data in 1993, though it resumed
requesting data in 1996. The authors repeated their experiment with
papers presented at the St.~Louis Federal Reserve Bank conference in
1992, and obtained similar response rates as those from the original JMCB Project. The
flagship economics journal, the \emph{American Economic Review} (AER),
did not start requesting data until 2003. Then after a 2003 article
showed that nonlinear maximization methods often produce wildly
different estimates across different software packages, that not a
single AER article tested their solution with different software, and
that fully half of queried authors from a chosen issue of the AER,
including a then editor of the journal, failed to comply with the policy
of providing data and code, editor Ben Bernanke made the data and code
policy mandatory in 2004 \citep{mccullough_verifying_2003, Bernanke2004}.

The current data policy from the \emph{American Economic Review} can be
seen here: \url{https://www.aeaweb.org/aer/data.php}. The AER conducted a self-review and
found good, but incomplete, compliance \citep{glandon_report_2010}; others believe much work remains \citep{anderson_role_2008}. In addition to all
the journals published by the American Economic Association, several top journals now explicitly require data and code to be
submitted at the time of publication. The last of the ``top 5'' general interest journals, \textit{The Quarterly Journal of Economics} adopted a data-sharing requirement in 2016.


\subsubsection{General Repositories}\label{general-repositories}

The previous section on the \emph{JMCB} describes only a few journals in
one field of the social sciences. Even if the journal to which you
submit your research does not require you to supply them with your code
and data, researchers should still share these materials. Though some
repositories, particularly Harvard's Dataverse, seem equipped to handle
data from practically any researcher (a free 1 TB of storage is
standard, with more possible upon request), many repositories specialize. \href{http://www.re3data.org}{The Registry of Research Data Repositories}
has described over 900 data repositories to help you find the right data
repository for your data. A key advantage to using a trusted repository
such as one listed there, in lieu of simply posting the data on your
own website or making your Dropbox folder public, is that many of these
repositories will take your data in its proprietary (Stata, SAS, SPSS,
etc.) form, and make it accessible in other formats. Storing your data in a repository with other similar datasets also makes it easier for others to find your data, instead of requiring that they already know of its existence, as would likely be the case with personal websites. Your own personal website is also more likely to be taken offline, should a researcher change schools or retire.

\subsubsection{Differential Privacy}\label{differential-privacy}

One important caveat to making data widely available is that despite
anonymization, in the age of big data, sometimes individual subjects can
easily be identified. \cite{heffetz_privacy_2014} recount deliberate data
releases by Yahoo! Inc., the Massachusetts state government, and
Netflix, that could easily be used to identify individuals in the data,
despite the absence of direct identifiers such as names or social
security numbers. The problem is that ``de-identification does not
guarantee anonymization.'' This problem is well known in computer science, but solutions are still being developed and are not widely implemented.


\subsection{Reporting Standards}\label{reporting-standards}
In research, the devil is in the details. Whether assessing the validity of a research design or attempting to replicate a study, details of what exactly was done must be recorded and made available to other researchers. The exact details that are relevant will likely differ from field to field, but an increasing number of fields have
produced centralized checklists that describe (in excruciating detail) what disclosure is required of published studies. These checklists are not often published with the paper, but can be submitted with the original article so that reviewers can check that it has been completed. With nearly infinite and easy web storage, researchers can easily post these materials in a repository even if journal editors insist on cutting their methods sections for space reasons. 

\subsubsection{Randomized Trials and CONSORT}\label{randomized-trials}
The most widely adopted reporting standard guideline is the Consolidated Standards of Reporting Trials (\href{http://www.consort-statement.org}{CONSORT}). Reporting standards evolved parallel to construction of \href{http://clinicaltrials.gov}{clinicaltrials.gov} and registration, and are now nearly universally adopted for randomized trials published in medical journals, required or requested by reviewers during the review process. This is still in its infancy in the social sciences.

The original CONSORT  was developed in the mid 1990's \citep{begg_c_improving_1996}. After five years, research showed that reporting of essential details had significantly increased in journals requiring the standard  \citep{moher_d_use_2001}. The statement was revised in 2001, and again in 2010  \citep{moher_consort_2001, schulz_consort_2010}. The statement is a 25-item checklist pertaining to the title, abstract, introduction, methods, results, and discussion of the article in question, and seeks to delineate the minimum requirements of disclosure that may not be sufficiently addressed through other measures.

\subsubsection{Social Science Reporting Standards}\label{soc-sci-standards}
Though a standard akin to CONSORT has not been formally adopted by social science or economics journals, at least as far as we are aware, there have been attempts to do this: in political science, the Experimental Research Section Standards Committee produced a detailed list of items required for disclosure of experiments \citep{gerber_reporting_????}. This checklist is available \href{http://www.davidhendry.net/research-supplemental/gerberetal2014-reportingstandards/gerberetal2014-reportingstandards&appendix1.pdf}{here}.\footnote{\url{http://www.davidhendry.net/research-supplemental/gerberetal2014-reportingstandards/gerberetal2014-reportingstandards&appendix1.pdf}} In economics, one article has highlighted the fact that there is limited discussion of essential features of randomization (How was randomization stratified, if at all? How were control variables determined?), but no standards have been adopted \citep{bruhn_pursuit_2009}. In psychological and behavioral research, an extention to CONSORT for Social and Psychological Interventions (CONSORT-SPI) has been developed in \citep{montgomery2013protocol}, but so far has not been widely adopted nor required by journals. 

\subsubsection{Observational Reporting Standards}\label{observational-standards}
Social science has yet to make a serious push for reporting standards in RCTs, let alone observational work, but the medical/epidemiological literature has created standards in this type of work, though they are not as widely adopted as CONSORT. Perhaps the most well-known is the \href{http://www.strobe-statement.org}{STROBE Statement} (Strengthening the Reporting of Observational Studies in Epidemiology)\citep{strobestatement2007}. STROBE provides checklists for reporting of cohort, case-control, and cross-sectional studies. These standards have been endorsed by approximately 100 journals in the field.\footnote{\url{http://www.strobe-statement.org/index.php?id=strobe-endorsement}}

Medicine has in fact come up with too many checklists to describe them all individually. Acknowledging that every field and type of research is different, the Equator Network (Enhancing the Quality of Transparency of Health Research) serves as an umbrella organization that seeks to keep tabs on all the best reporting standards and help researchers find which reporting standard is most relevant for their research.\footnote{See \url{http://www.equator-network.org/} for more information.}





\section{Conclusion}\label{conclusion}

If science progresses by standing on the shoulders of giants, then good science requires research to be conducted in a transparent and reproducible fashion. This may require extra up-front work by researchers compared to the current state of affairs. Before one runs an experiment, researchers could write down their hypothesis, carefully explain how they are going to test the hypothesis, perhaps going as far as writing down the very regression analysis specification they plan to run, write a detailed protocol of the exact experimental setting, and then post all of this publicly on the Internet in a public repository. In the long run, however, science will reap a reward greater than these costs. Statistical analysis will result in p-values that can be interpreted as intended, and in an appropriate publication bias context. Replicating the work of other researchers will be easy, because their data and code will be in a public repository, and code could be well documented and understandable, and written to easily recreate the original results, ideally with a single click. Science could potentially move forward at a faster rate.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\singlespacing
%\section{References}\label{references}
\bibliographystyle{plainnat}
\bibliography{Bibliography}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}
